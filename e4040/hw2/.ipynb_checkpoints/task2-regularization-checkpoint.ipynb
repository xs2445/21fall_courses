{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Task 2: Regularizations\n",
    "\n",
    "In this task, you are going to experiment with two popular regularization methods. \n",
    "\n",
    "**Batch normalization:** Batch normalization makes it possible to train a deep network. When a network becomes deeper, the distribution of hidden neurons' values will also shift greatly, and that is one reason that makes it difficult to train a deep neural network. Machine learning taught us that normalization is a good preprocessing method to deal with such a problem. Therefore, batch normalization deploys a similar idea in the neural network by re-normalizing the hidden values of each layer before transfering values to the next layer.\n",
    "\n",
    "**Dropout:** In the last assignment, you trained a shallow network and everything looked fine. However, when the network becomes larger and deeper, it will also become harder to train. The first potential problem is over-fitting, that is, the network overreacts to noise or random errors of the training data while failing to detect the underlying distribution pattern. It is more likely to occur when the model becomes complex and contains more trainable parameters. Dropout is a well-known method that can eliminate such effects. The core idea behind it is quite simple: rather than updating all trainable parameters each time, it randomly selects a subset of parameters to update and keeps other parameters unaltered.\n",
    "\n",
    "* References\n",
    "    * https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (60000, 784)\n",
      "Training labels shape:  (60000,)\n",
      "Validation data shape:  (10000, 784)\n",
      "Validation labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, val = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_val_raw, y_val = val\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_val = X_val_raw.reshape((X_val_raw.shape[0], X_val_raw.shape[1]**2))\n",
    "\n",
    "#Index from the 10000th image of the dataset\n",
    "# X_val = X_train[10000:10500,:]\n",
    "# y_val = y_train[10000:10500]\n",
    "# X_train = X_train[10500:12500,:]\n",
    "# y_train = y_train[10500:12500]\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32) - mean_image\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# We've vectorized the data for you. That is, we flatten the 32×32×3 images into 1×3072 Numpy arrays.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement batch normalization foward function\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span> Edit functions **bn_forward** in **./utils/reg_funcs.py**. <br> If the code is running correctly, **mean of a2_bn for train will be very close to 0 and variance of a2_bn will be close to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of a2:  [-3.71855056  3.21810258 -2.44388986]\n",
      "var of a2:  [45.32564755 70.17112882 32.35120115]\n",
      "(train) mean of a2_bn:  [-1.93178806e-16 -1.84297022e-16 -1.12132525e-16]\n",
      "(train) var of a2_bn:  [0.99999978 0.99999986 0.99999969]\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# Checking/verification code. Don't change it.     #\n",
    "####################################################\n",
    "\n",
    "from utils.reg_funcs import bn_forward\n",
    "from utils.reg_funcs import bn_backward\n",
    "\n",
    "np.random.seed(1338)\n",
    "N, D, H1, H2 = 300, 20, 8, 3\n",
    "x_in = np.random.randn(N, D)\n",
    "w1 = np.random.randn(D,H1)\n",
    "w2 = np.random.randn(H1,H2)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "\n",
    "# Before batch normalization\n",
    "print(\"mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"var of a2: \", np.var(a2, axis=0))\n",
    "\n",
    "# Test \"train mode\" of forward function\n",
    "# After batch normalization, the mean should be close to zero and var should be close to one. \n",
    "bn_config = {\"epsilon\":1e-5, \"decay\":0.9}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "print(\"(train) mean of a2_bn: \", np.mean(a2_bn, axis=0))\n",
    "print(\"(train) var of a2_bn: \", np.var(a2_bn, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Test \"moving average\" and \"test mode\" of the bn_forward function. <br> We expect that the test mean & variance of a2 to be fairly close to the real values and the test mean & variance of a2_bn to be close to 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real mean of data:  [-4.59744803  3.61134729 -2.80652811]\n",
      "real var of data:  [52.92659067 69.42182711 34.21217349]\n",
      "moving mean of data:  [[-4.52108283  3.6609178  -2.85845037]]\n",
      "moving var of data:  [[53.60902501 69.9638493  33.22644173]]\n",
      "********************************************************************************\n",
      "(test) mean of a2:  [-4.44083807  3.31384196 -3.00030295]\n",
      "(test) var of a2:  [46.19972406 61.65712451 33.86251068]\n",
      "(test) mean of a2_bn:  [ 0.01095968 -0.04149421 -0.02460907]\n",
      "(test) var of a2_bn:  [0.86178988 0.88127106 1.01914315]\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# Checking code. Don't change it.     #\n",
    "#######################################\n",
    "\n",
    "# Test \"moving average\" and \"test mode\" of forward function\n",
    "# Then you are going to run the forward function under \"training mode\" for several times, \n",
    "# and the moving mean and moving var will be close to the real mean and var of the input data.\n",
    "# Next, run the forward function under \"test\" mode and you will see that the mean and var of its \n",
    "# output will be also close to gamma, beta that you have set before.\n",
    "\n",
    "bn_config = {\"epsilon\":1e-5, \"decay\":0.9}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "\n",
    "# collect_data: for calculating real mean and var of a2 later.\n",
    "collect_data = a2\n",
    "for _ in range(100):\n",
    "    x_in = np.random.randn(N, D)\n",
    "    a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "    collect_data = np.concatenate((collect_data, a2), axis=0)\n",
    "    bn_forward(a2, gamma, beta, bn_config, \"train\")\n",
    "\n",
    "# compare moving_mean and moving_var with real mean and var.\n",
    "# You should see that they are close to each other.\n",
    "print(\"real mean of data: \", np.mean(collect_data, axis=0))\n",
    "print(\"real var of data: \", np.var(collect_data, axis=0))\n",
    "print(\"moving mean of data: \", bn_config[\"moving_mean\"])\n",
    "print(\"moving var of data: \", bn_config[\"moving_var\"])\n",
    "\n",
    "# \"test mode\" of forward function\n",
    "# After bn_forward, the mean and var of output should be kind of close to gamma and beta.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "print(\"*\"*80)\n",
    "print(\"(test) mean of a2: \", np.mean(a2, axis=0))\n",
    "print(\"(test) var of a2: \", np.var(a2, axis=0))\n",
    "a2_bn, _ = bn_forward(a2, gamma, beta, bn_config, \"test\")\n",
    "print(\"(test) mean of a2_bn: \", np.mean(a2_bn, axis=0))\n",
    "print(\"(test) var of a2_bn: \", np.var(a2_bn, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Use TensorFlow 2 functions to verify the correctness of the backward function. You should use functions from TensorFlow 2 to get the gradients. \n",
    "<br>**Hint: use tf.GradientTape()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Is a2_bn correct? True\n",
      "Is da2 correct? True\n",
      "Is dgamma correct? True\n",
      "Is dbeta correct? True\n"
     ]
    }
   ],
   "source": [
    "# After verifying the forward function and save the bn_config.\n",
    "x_in = np.random.randn(N, D)\n",
    "a2 = np.maximum(x_in.dot(w1),0).dot(w2)\n",
    "da2_bn = np.ones_like(a2)\n",
    "# Test backward function with tensorflow\n",
    "# You will use bn_config = {\"eps\":1e-5, \"decay\":0.9, \"moving_mean\":moving_mean, \"moving_var\":moving_var}\n",
    "gamma = np.ones(H2)\n",
    "beta = np.zeros(H2)\n",
    "a2_bn, cache = bn_forward(a2, gamma, beta, bn_config, \"test\")\n",
    "\n",
    "da2, dgamma, dbeta = bn_backward(da2_bn, cache)\n",
    "\n",
    "###################################################\n",
    "# TODO: verify the backward code. You should use  #\n",
    "# the parameters in bn_config and other variables #\n",
    "# above.                                          #\n",
    "###################################################\n",
    "\n",
    "a2_tf = tf.Variable(a2)\n",
    "gamma_tf = tf.Variable(gamma)\n",
    "beta_tf = tf.Variable(beta)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(a2_tf)\n",
    "    tape.watch(gamma_tf)\n",
    "    tape.watch(beta_tf)\n",
    "    a2_bn_tf = tf.nn.batch_normalization(\n",
    "        a2_tf,\n",
    "        # mean=tf.math.reduce_mean(a2, axis=0),\n",
    "        mean=bn_config[\"moving_mean\"],\n",
    "        # variance=tf.math.reduce_variance(a2, axis=0),\n",
    "        variance=bn_config[\"moving_var\"],\n",
    "        offset=beta_tf,\n",
    "        scale=gamma_tf,\n",
    "        variance_epsilon=bn_config[\"epsilon\"]\n",
    "    )\n",
    "    da2_tf = tape.gradient(a2_bn_tf, a2_tf)\n",
    "    dgamma_tf = tape.gradient(a2_bn_tf, gamma_tf)\n",
    "    dbeta_tf = tape.gradient(a2_bn_tf, beta_tf)\n",
    "    \n",
    "a2_bn_check = a2_bn_tf.numpy()\n",
    "da2_check = da2_tf.numpy()\n",
    "dgamma_check = dgamma_tf.numpy()\n",
    "dbeta_check = dbeta_tf.numpy()\n",
    "\n",
    "\n",
    "###################################################\n",
    "# ENDTODO #\n",
    "###################################################\n",
    "    \n",
    "# Make comparison\n",
    "print(\"Is a2_bn correct? {}\".format(np.allclose(a2_bn, a2_bn_check)))\n",
    "print(\"Is da2 correct? {}\".format(np.allclose(da2, da2_check)))\n",
    "print(\"Is dgamma correct? {}\".format(np.allclose(dgamma, dgamma_check)))\n",
    "print(\"Is dbeta correct? {}\".format(np.allclose(dbeta, dbeta_check)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 3.44222313  4.64983166 34.9310013 ], shape=(3,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(dgamma_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add batch normalization into MLP. Please understand **loss**, **predict** functions of the class **MLP** in **./utils/neuralnets/mlp.py**\n",
    "\n",
    "2. First create a shallow MLP like two-layer network like [100,10]. Train it without and with batch normalization. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "3. Then, create a 5-layer MLP network like [100,50,50,50,10] and train the network with and without batch normalization. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "4. Make a comparison and describe what you have found in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on shallow MLP** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.neuralnets.mlp import MLP \n",
    "from utils.optimizers import AdamOptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 600\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'bn_gamma_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-2d4dbf644c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m hist_shallow_no_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n\u001b[1;32m      9\u001b[0m                            \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                            verbose=False, record_interval = 4)\n\u001b[0m",
      "\u001b[0;32m~/codes/test/e4040/hw2/utils/optimizers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, X_train, y_train, X_valid, y_valid, num_epoch, batch_size, learning_rate, learning_decay, verbose, record_interval)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# update model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codes/test/e4040/hw2/utils/neuralnets/mlp.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m###############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bn_gamma_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bn_beta_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bn_gamma_0'"
     ]
    }
   ],
   "source": [
    "# Build a two-layer network without batch normalization.\n",
    "# Here is a demo.\n",
    "use_bn = False\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_shallow_no_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=1e-3, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a two-layer network with batch normalization. Remember to \"use_bn\".\n",
    "use_bn = True\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_shallow_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=1e-3, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bn and no_bn results together in three plots and make a comparison. \n",
    "title_name = [\"loss\", \"train acc\", \"val acc\"]\n",
    "_, axarr = plt.subplots(1,3, figsize=(15,5))\n",
    "for i in range(3):\n",
    "    axarr[i].plot(hist_shallow_no_bn[i], label=\"no_bn\")\n",
    "    axarr[i].plot(hist_shallow_bn[i], label=\"bn\")\n",
    "    axarr[i].legend(), axarr[i].set_title(title_name[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment on deep MLP** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deep 5-layer network without batch normalization. Remember to \"use_bn\".\n",
    "use_bn = False\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 50, 50, 50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_no_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=2e-4, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deep 5-layer network with batch normalization. Remember to \"use_bn\".\n",
    "use_bn = True\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 50, 50, 50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=100, learning_rate=2e-4, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bn and no_bn results together in three plots and make a comparison. \n",
    "title_name = [\"loss\", \"train acc\", \"val acc\"]\n",
    "_, axarr = plt.subplots(1,3, figsize=(15,5))\n",
    "for i in range(3):\n",
    "    axarr[i].plot(hist_deep_no_bn[i], label=\"no_bn\")\n",
    "    axarr[i].plot(hist_deep_bn[i], label=\"bn\")\n",
    "    axarr[i].legend(), axarr[i].set_title(title_name[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement dropout_forward function\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span> Edit function **dropout_forward** in **./utils/reg_funcs.py**. If the code is running correctly, you will see that the outputs of the verification code should be close to each other. \n",
    "<br><br> If the function is correct, then **the output mean should be close to the input mean. Input mean and output test mean should be identical.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = np.random.randn(500, 500) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Checking/verification code. Don't change it. #\n",
    "################################################\n",
    "from utils.reg_funcs import dropout_forward\n",
    "from utils.reg_funcs import dropout_backward\n",
    "\n",
    "p = 0.7\n",
    "dropout_config = {\"enabled\": True, \"keep_prob\": p}\n",
    "# feedforward\n",
    "out, cache = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"train\")\n",
    "out_test, _ = dropout_forward(x=x_in, dropout_config=dropout_config, mode=\"test\") \n",
    "# backward\n",
    "dout = np.ones_like(x_in)\n",
    "dx = dropout_backward(dout, cache)\n",
    "################################################\n",
    "# Checking/verification code. Don't change it. #\n",
    "################################################\n",
    "# Check forward correctness\n",
    "print(\"mean_of_input = {}\".format(p*np.mean(x_in)))\n",
    "print(\"mean_of_out = {}\".format(np.mean(out)))\n",
    "print(\"mean_of_out_test = {}\".format(np.mean(out_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Experiments with MLP\n",
    "\n",
    "<span style=\"color:red\"><strong>TODO:</strong></span>\n",
    "\n",
    "1. Add dropout into the MLP. Review **loss**, **predict** functions of class **MLP** in **./utils/neuralnets/mlp.py** and understand how the dropout is added into the MLP.\n",
    "\n",
    "2. Customize your own MLP network.Then, train networks with different **keep_prob**, like 0.1, 0.3, 0.5, 0.7, 0.9, 1. If **keep_prob**==1, then the network is the MLP without dropout.\n",
    "\n",
    "3. Plot the loss, training accuracy, and validation accuracy curves.\n",
    "\n",
    "Note that checking/validation code is included below with preselected dropout parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example on how to collect loss and accuracy info\n",
    "dropout_config = {\"enabled\":True, \"keep_prob\": 1}\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_no_dropout = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.9\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_9 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.7\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_7 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.5\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_5 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_config[\"keep_prob\"] = 0.3\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_3 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small retention rate\n",
    "dropout_config[\"keep_prob\"] = 0.1\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200], num_classes=10, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_dropout_1 = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=2, batch_size=100, learning_rate=1e-3, learning_decay=0.95,\n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_no_dropout, train_acc_no_dropout, val_acc_no_dropout = hist_no_dropout\n",
    "loss_dropout_1, train_acc_dropout_1, val_acc_dropout_1 = hist_dropout_1\n",
    "loss_dropout_3, train_acc_dropout_3, val_acc_dropout_3 = hist_dropout_3\n",
    "loss_dropout_5, train_acc_dropout_5, val_acc_dropout_5 = hist_dropout_5\n",
    "loss_dropout_7, train_acc_dropout_7, val_acc_dropout_7 = hist_dropout_7\n",
    "loss_dropout_9, train_acc_dropout_9, val_acc_dropout_9 = hist_dropout_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_no_dropout, label=\"no dropout\")\n",
    "plt.plot(loss_dropout_1, label=\"keep_prob=0.1\")\n",
    "plt.plot(loss_dropout_3, label=\"keep_prob=0.3\")\n",
    "plt.plot(loss_dropout_5, label=\"keep_prob=0.5\")\n",
    "plt.plot(loss_dropout_7, label=\"keep_prob=0.7\")\n",
    "plt.plot(loss_dropout_9, label=\"keep_prob=0.9\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc_no_dropout, label=\"no dropout\")\n",
    "plt.plot(train_acc_dropout_1, label=\"keep_prob=0.1\")\n",
    "plt.plot(train_acc_dropout_3, label=\"keep_prob=0.3\")\n",
    "plt.plot(train_acc_dropout_5, label=\"keep_prob=0.5\")\n",
    "plt.plot(train_acc_dropout_7, label=\"keep_prob=0.7\")\n",
    "plt.plot(train_acc_dropout_9, label=\"keep_prob=0.9\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_acc_no_dropout, label=\"no dropout\")\n",
    "plt.plot(val_acc_dropout_1, label=\"keep_prob=0.1\")\n",
    "plt.plot(val_acc_dropout_3, label=\"keep_prob=0.3\")\n",
    "plt.plot(val_acc_dropout_5, label=\"keep_prob=0.5\")\n",
    "plt.plot(val_acc_dropout_7, label=\"keep_prob=0.7\")\n",
    "plt.plot(val_acc_dropout_9, label=\"keep_prob=0.9\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO:</strong></span> Describe what you find in this dropout experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Dropout + Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep network with both dropout and batch normalization.\n",
    "dropout_config = {\"enabled\":True, \"keep_prob\": 0.7}\n",
    "use_bn = True\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[100, 50], num_classes=10, \n",
    "            weight_scale=1e-3, l2_reg=0.0, dropout_config=dropout_config, use_bn=use_bn)\n",
    "optimizer = AdamOptim(model)\n",
    "\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_deep_bn = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=5, batch_size=200, learning_rate=1e-3, learning_decay=1.0, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks https://arxiv.org/abs/1602.07868\n",
    "* Highway netowrk https://arxiv.org/pdf/1505.00387.pdf"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50286e954820f4eea984950900e1b0d0edcf026073fd0a170f2369702fdf1a29"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
