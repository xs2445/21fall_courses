{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "A 2 layer MLP is sufficient to model most functions. Why are deep networks used instead of 2 layer networks? \n",
    "\n",
    "   Your answer: **[The goal of Neural Networks is to imitate an desired function. According to the book Deep Learning, the universal approximation theorem of the neural networks states that a feedforward network with a linear output layer and at least one hidden layer with any \"squashing\" activation function (such as sigmoid) can approximate any Borel measurable function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units. There are two things needs to be highlighted here. One is that a 2 layer MLP is a universal approximator, so the accuracy is guaranteed. The other is the 2 layer MLP can approximate any function given enough hidden units, so the efficiency is uncertain. In practice, if we want to train the network to learn a very complicated pattern, it is almost impossible to have a high accuracy with only one hidden layer. It will need infinite nodes in the hidden layer and no computer can handle it. On the other hand, if we go deep, then the network can have more nonlinear combinations, which can make the estimation more efficient]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "What are the differences between stochastic gradient descent and Batch gradient descent? Why is mini-batch gradient descent used in practice?\n",
    "\n",
    "   Your answer: **[Batch Gradient Descent method takes the average of the gradients of the whole dataset in a single step. Stochastic Gradient Descent methos takes a single randomly selected subset or element of the dataset for training. The gradient of Batch Gradient method is the 'real' gradient, and the gradient of SGD is an estimation of that real gradient. Using the BGD method can be more accurate. However, in practice, we usually tend to collect and create a huge dataset for a better description of the pattern we want to learn. The whole dataset will be too large to compute in one step. This is why mostly mini-batch is used in practice. Although the mini-batch or SGD method is an estimation of the real gradient, by the law of large number, the more epochs we train, the more estimation will close to the real gradient.]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why are activation functions used in deep learning? What are the issues that can arise when using sigmoid or tanh activation functions?\n",
    "\n",
    "   Your answer: **[The neural networks without activation function is basiclly linear transmation from the dimension of input to the dimension of output. In the real world, most problems are nonlinear, so we need to add nonlinear elements which is activation function to make neural network have the ability to imitate a nonlinear function. The sigmoid and tanh both map the input data from (-infinite,+infinite) to (c1,c2), where c1 and c2 are both constant. However, both sigmoid and tanh are sensitive to input data only close to zero. If the input data is large, no matter how large it is, it will close to c1 or c2, which make no difference for sigmoid and tanh.]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "What are the differences between linear and logistic regression? Are both linear?\n",
    "\n",
    "   Your answer: **[Linear regression is used to predict the value of continous variables. And Logistic regression is used for solving classification problems. Also, logistic regression is not only a linear transformation, it need to use a sigmoid function after the linear transformation to evaluate the probability of being each catagory. So Linear regressiong is linear and logistic regreesion is not linear.]**\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: **[The performance of the default setting is already quite well, but still have improving space. Fisrtly, I directly tought of the number of hidden nodes. I increased the number of hidden nodes. However I found that there is no guarantee for a better accuracy if I increase the number of nodes. I made the number of hidden nodes to 1000. Then I found that the test accuracy is quite lower than the training accuracy, which means a little bit of over fitting. So I increased the regularization constant and the gap between traning and testing result became much smaller. The network converged very quick and didn't improve after a certain epoch. That means that it stucked in the local minima. I decreased the learning rate and increased the number of batchsize and epochs, the network converged much slower and had a better accuracy.]**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: **[t-Distributed Stochastic Neighbor Embedding is a technique for dimension reduction and is well suited for visualization of high-dimensional datasets. For a better illustration of the dataset, we need to tune the parameters of t-SNE. Perplexity basically means the \"distance\" to regional density variation in the dataset. So with a lower perplecity, the data seems to mixed togather. Also after the MLP. the distance between different catagories is much more larger.]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
