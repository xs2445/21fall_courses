{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7OSce2LHK_B"
   },
   "source": [
    "# Assignment 1, Task 1 - Basic ML Classifiers\n",
    "\n",
    "In this task, you are going to implement two classifers and apply them to the Fashion-MNIST dataset: \n",
    "\n",
    "(1) Logistic regression classifier\n",
    "\n",
    "(2) Softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mLE5oHAkHK_C"
   },
   "outputs": [],
   "source": [
    "# Import modules, make sure you have installed all required packages before you start.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMZSC2GLHK_F"
   },
   "source": [
    "## Load Fashion-MNIST data\n",
    "\n",
    "Fashion-MNIST is a widely used dataset mainly used for benchmarking machine learning models. Images are drawn from Zalando's clothing articles and the dataset consists of a training set with 60,000 examples and a test set with 10,000 examples. Each example is a 28x28 pixel grayscale image with an associated label from 10 classes. We will use this to create our training set, validation set, and test set.\n",
    "\n",
    "See https://github.com/zalandoresearch/fashion-mnist for more details on Fashion-MNIST. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-class dataset Fashion-MNIST\n",
    "First, we load the raw Fashion-MNIST data to create a 10-class dataset and manually define a label map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7274,
     "status": "ok",
     "timestamp": 1559884655914,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "bV0hpeEqHK_G",
    "outputId": "c4b54f60-33c3-4d00-c783-316c1de30020",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, test = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_test_raw, y_test = test\n",
    "\n",
    "# the integer labels in y_train and y_test correspond to the index of this label map\n",
    "label_map = ['t-shirt/top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "# Here we vectorize the data (rearranged the storage of images) for you. \n",
    "# That is, we flatten 1×28×28 images into 1×784 Numpy arrays.\n",
    "# The reason we do this is because we can not put 2-D image representations into our model. \n",
    "# This is common practice (flattening images before putting them into the ML model). \n",
    "# Note that this practice may not be used for Convolutional Neural Networks (CNN). \n",
    "# We will later see how we manage the data when used in CNNs in later assignments.\n",
    "\n",
    "# Check the results\n",
    "print('Raw training data shape: ', X_train_raw.shape)\n",
    "print('Raw test data shape: ', X_test_raw.shape)\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_test = X_test_raw.reshape((X_test_raw.shape[0], X_test_raw.shape[1]**2))\n",
    "\n",
    "print('Vectorized training data shape: ', X_train.shape)\n",
    "print('Vectorized test data shape: ', X_test.shape)\n",
    "\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8435,
     "status": "ok",
     "timestamp": 1559884658097,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "6n4v89BcHK_L",
    "outputId": "9141d16e-b67a-4d60-a7ed-0eb350bc4748",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to reshape vectorized data into the image format for visualization\n",
    "X = X_train.reshape(X_train.shape[0], X_train_raw.shape[1], X_train_raw.shape[2])\n",
    "\n",
    "print(label_map)\n",
    "print(X.shape)\n",
    "\n",
    "#Visualizing Fashion-MNIST data. We randomly choose 25 images from the train dataset.\n",
    "fig, axes1 = plt.subplots(5,5,figsize=(8,8))\n",
    "for j in range(5):\n",
    "    for k in range(5):\n",
    "        i = np.random.choice(range(len(X)))\n",
    "        axes1[j][k].set_axis_off()\n",
    "        axes1[j][k].imshow(X[i:i+1][0], cmap='gray')\n",
    "        axes1[j][k].set_title(label_map[y_train[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1559884658978,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "6YoejUt3HK_O",
    "outputId": "8f961d92-4960-4fb2-d496-5e0521996c7a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data organization:\n",
    "#    Train data: 49,000 samples from the original train set: indices 1~49,000\n",
    "#    Validation data: 1,000 samples from the original train set: indices 49,000~50,000\n",
    "#    Test data: 1,000 samples from the original test set: indices 1~1,000\n",
    "#    Development data (for gradient check): 100 random samples from the train set: indices 1~49,000\n",
    "#    Development data (binary) (only for gradient check in Part 1): 100 random samples from the subsampled binary train set\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 100\n",
    "num_dev_binary = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "X_test = X_test[:num_test, :]\n",
    "y_test = y_test[:num_test]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-class dataset\n",
    "Next, in order to implement the experiment with the logistic regression classifier, we subsample the 10-class dataset to the 2-class dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subsample 10-class training set to 2-class training set\n",
    "X_train_binary = X_train[y_train<2,:]\n",
    "num_training_binary = X_train_binary.shape[0]\n",
    "y_train_binary = y_train[y_train<2]\n",
    "mask_binary = np.random.choice(num_training_binary, num_dev_binary, replace=False)\n",
    "\n",
    "X_val_binary = X_val[y_val<2,:]\n",
    "y_val_binary = y_val[y_val<2]\n",
    "\n",
    "X_dev_binary = X_train_binary[mask_binary]\n",
    "y_dev_binary = y_train_binary[mask_binary]\n",
    "\n",
    "\n",
    "print('Train data (binary) shape: ', X_train_binary.shape)\n",
    "print('Train labels (binary) shape: ', y_train_binary.shape)\n",
    "print('Validation data (binary) shape: ', X_val_binary.shape)\n",
    "print('Validation labels (binary) shape: ', y_val_binary.shape)\n",
    "print('Development data (binary) shape:', X_dev_binary.shape)\n",
    "print('Development data (binary) shape', y_dev_binary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2569,
     "status": "ok",
     "timestamp": 1559884662531,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "XWboKTBmHK_S",
    "outputId": "a81860fe-983b-44c7-8382-0b68b396815f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing: subtract the mean value across every dimension, for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension, for binary training data\n",
    "mean_image = np.mean(X_train_binary, axis=0)\n",
    "\n",
    "X_train_binary = X_train_binary.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val_binary = X_val_binary.astype(np.float32) - mean_image\n",
    "X_dev_binary = X_dev_binary.astype(np.float32) - mean_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_sQKxgGHK_V"
   },
   "source": [
    "## Part 1: Logistic Regression Classifier\n",
    "\n",
    "In this part, you are going to implement a logistic regression classifier. \n",
    "\n",
    "Let’s assume a training dataset of images $x_i \\in R^D$, each with an associated label $y_i$. We have that $i=1 \\dots N$ and $y_i \\in 1 \\dots K$, where **N** represents the number of examples (each with a dimensionality **D**) and **K** represents the number of distinct classes.\n",
    "\n",
    "#### **Start detailed data description** (feel free to skip if you feel you understand)\n",
    "\n",
    "    N: number of input images (examples)\n",
    "    D: dimension of one input image\n",
    "    K: number of classes\n",
    "\n",
    "For the Fashion-MNIST dataset we have training examples `X_train` with shape $(49,000, 784)$ and the corresponding labels `y_train` with shape $(4910,)$.\n",
    "\n",
    "Here number of input data (images) is: $N=49,000$, and each image has a flattened dimension: $D=784$ ($28\\times28$).\n",
    "\n",
    "So we have: $$X: N \\times D$$  $$y: N \\times 1$$\n",
    "\n",
    "Think about one single image in $X$, represented as: $x_i \\in R^D$ ($x_i: 1\\times D$)\n",
    "\n",
    "it is associated with a label $y_i$. Here $i=1 \\dots N$ and $y_i \\in 1 \\dots K$.\n",
    "\n",
    "#### **End detailed data description**\n",
    "\n",
    "We will now define the score function $f: R^D \\to R^K$ that maps the raw image pixels to class scores: $$f(x_i; W, b)=W x_i + b$$\n",
    "where $W$ is of size $D \\times K$ and $b$ is of size $1 \\times K$. \n",
    "\n",
    "Here we will use **bias trick** to represent the two parameters $W,b$ as one by extending the vector $x_i$ with one additional dimension that always holds the constant **1** - a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply: $$f = f(x_i;W)=W x_i$$\n",
    "\n",
    "For our data:\n",
    "\n",
    "$$D=784+1=785$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>:  Implement the bias trick by extending the feature vectors. In our case this involves adding a bias dimension of ones to each of `X_train`, `X_val`, `X_test`, `X_dev`, `X_train_binary`, `X_val_binary`, and `X_dev_binary`.\n",
    "\n",
    "<span style=\"color:red\"><strong>HINT</strong></span>: Make your life easier with `np.hstack`. https://numpy.org/doc/stable/reference/generated/numpy.hstack.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>SOLUTION</strong></span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# TODO: Append the bias dimension of ones (i.e. bias trick) so that our models\n",
    "# only have to worry about optimizing a single weight matrix W.\n",
    "# START of your code\n",
    "#############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# END of your code\n",
    "#############################################################\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"X_dev shape: {}\".format(X_dev.shape))\n",
    "print(\"X_train_binary shape: {}\".format(X_train_binary.shape))\n",
    "print(\"X_val_binary shape: {}\".format(X_val_binary.shape))\n",
    "print(\"X_dev_binary shape: {}\".format(X_dev_binary.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brief introduction to logistic regression classifier**\n",
    "\n",
    "Logistic regression classifier can solve a binary classification problem ($K=2$). A binary logistic regression classifier has only two classes (0,1), and calculates the probability of class 1 as:\n",
    "\n",
    "$$\n",
    "P(y=1 | x ; w)=\\frac{1}{1+e^{-f}}=\\sigma\\left(f\\right)\n",
    "$$\n",
    "\n",
    "Since the probabilities of class 1 and 0 sum to one, the probability for class 0 is:\n",
    "\n",
    "$$\n",
    "P(y=0 | x ; w)=1-P(y=1 | x ; w)\n",
    "$$\n",
    "\n",
    "Hence, an example is classified as a positive example ($y = 1$) if $\\sigma\\left(f\\right)>0.5$, or equivalently if the score $f>0$. The loss function then maximizes the log likelihood of this probability over all examples. You can convince yourself that this simplifies to:\n",
    "\n",
    "$$\n",
    "L_{i}=- (y_{i} \\log \\left(\\sigma\\left(f_{i}\\right)\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\sigma\\left(f_{i}\\right)\\right))\n",
    "$$\n",
    "\n",
    "and ultimately:\n",
    "\n",
    "$$ L = \\frac{1}{N}\\sum_i L_i + reg\\times\\|W\\|_2$$\n",
    "\n",
    "where the labels $y_{i}$ are assumed to be either 1 (positive) or 0 (negative), and $\\sigma(\\cdot)$ is the sigmoid function. The expression above can look scary, but its gradients with respect to $f$ and $W$ are simple and intuitive: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{i}}{\\partial f}=-(y_{i}-\\sigma\\left(f_{j}\\right))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{i}}{\\partial W}= - (y_{i}-\\sigma\\left(f_{j}\\right)) * x_{i}\n",
    "$$\n",
    "\n",
    "[1] http://cs231n.github.io/neural-networks-2/\n",
    "\n",
    "[2] https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_sQKxgGHK_V"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete the code in **./utils/classifiers/logistic_regression.py**. You have to implement logistic regression in two ways: \n",
    "naive and vectorized.\n",
    "We provide the verification code for you to check if your code works properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>NOTE</strong></span>: Please do not change the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1559884668395,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "v0DB0KWoHK_Y",
    "outputId": "eb47b0b6-6b34-4625-9a24-03dc219914fd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verification code for checking the correctness of the implementation of logistic_regression\n",
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "from utils.classifiers.logistic_regression import logistic_regression_loss_naive\n",
    "from utils.classifiers.logistic_regression import logistic_regression_loss_vectorized\n",
    "\n",
    "# generate a random weight matrix seeded with small numbers\n",
    "np.random.seed(3456)\n",
    "W = np.random.randn(X_train.shape[1], 1) * 0.0001  # generate initial weight vector with shape of feature vector transposed\n",
    "\n",
    "## naive numpy implementation of Logistic Regression\n",
    "loss_naive, grad_naive = logistic_regression_loss_naive(W, X_dev_binary, y_dev_binary, 0.000005)\n",
    "print('naive numpy loss: {}.'.format(loss_naive))\n",
    "\n",
    "## vectorized numpy implementation of Logistic Regression\n",
    "loss_vec, grad_vec = logistic_regression_loss_vectorized(W, X_dev_binary, y_dev_binary, 0.000005)\n",
    "print('vectorized numpy loss: {}.'.format(loss_vec))\n",
    "\n",
    "## check the correctness\n",
    "print('*'*100)\n",
    "print('Relative loss error is {}'.format(abs(loss_vec-loss_naive)))\n",
    "grad_err = np.linalg.norm(grad_naive - grad_vec)#, ord='fro')\n",
    "print('Relative gradient error is {}'.format(grad_err))\n",
    "print('*'*100)\n",
    "print('Is vectorized loss correct? {}'.format(np.allclose(loss_naive, loss_vec)))\n",
    "print('Is vectorized gradient correct? {}'.format(np.allclose(grad_naive, grad_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7u9-MqILHK_b"
   },
   "source": [
    "## Part 2: Softmax Classifier\n",
    "\n",
    "The softmax classifier generalizes the logistic regression classifier to multiple classes.\n",
    "\n",
    "In the softmax classifier, the function mapping $f(x_i;W)=W x_i$ stays unchanged, but we now interpret the obtained scores as the unnormalized log probabilities for each class (instead of using the sigmoid function to yeild probabilities), and replace the logistic regression loss with a cross-entropy loss that has the form: $$L_i= - \\log (\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}).$$\n",
    "\n",
    "The cross-entropy between a “true” distribution $p$ and an estimated distribution $q$ is defined as: $$H(p, q)=- \\sum_x p(x) \\log q(x).$$\n",
    "\n",
    "Now, let's rewrite the expression of $L_i$: $$L_i= - \\sum_k p_{i,k} \\log (\\frac{e^{f_k}}{\\sum_j e^{f_j}})$$\n",
    "where $p_i=[0, \\dots,1, \\dots, 0]$ contains a single 1 at the $y_i$-th position, $p_{i,k}=p_i[k]$, $p_i \\in [1 \\times K]$.\n",
    "\n",
    "**Note:** Numerical stability. When you are writing code for computing the Softmax function in practice, the intermediate terms $e^{f_{y_i}}$ and $\\sum_j e^{f_j}$ may be very large due to the exponentials. Dividing with large numbers can be numerically unstable, so it is important to use the normalization trick. Notice that if we multiply both the top and the bottom of the fraction by constant $C$ and push $C$ inside the exponent, we get the following (mathematically equivalent) expression: $$\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}=\\frac{Ce^{f_{y_i}}}{C\\sum_j e^{f_j}}=\\frac{e^{f_{y_i}+\\log C}}{\\sum_j e^{f_j+\\log C}}.$$\n",
    "\n",
    "A common choice for $C$ is to set it to $\\log C= -\\max_j f_j$.\n",
    "\n",
    "In most cases, you also need to consider a bias term $b$ with length D. However, in this experiment, since a bias dimension has been added into the $X$, you can ignore it. \n",
    "\n",
    "**Softmax derivations (in matrix representation)**\n",
    "\n",
    "$$\\nabla_{W_k} L= - \\frac{1}{N} \\sum_i x_i^T(p_{i,m} - P_m) + 2 \\lambda W_k,$$\n",
    "where $P_k= \\frac{e^{f_k}}{\\sum_j e^{f_j}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7u9-MqILHK_b"
   },
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete the code in **./utils/classifiers/softmax.py**.You have to implement the softmax layer in three ways. For the first two implementations, we provide the verification code for you to check if your implementation is correct. \n",
    "\n",
    "* Naive method using for-loop\n",
    "* Vectorized method\n",
    "\n",
    "Do not forget the $L_2$ regularization term in the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax in Tensorflow is demonstrated in the verification code below. This step will familiarize you with TensorFlow functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>NOTE</strong></span>: Please do not change the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1559884674707,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "DH7HdbEAHK_b",
    "outputId": "a6d3eb81-af5a-427e-f25e-4c811e36d847",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verification code for checking the correctness of the implementation of softmax implementations\n",
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "from utils.classifiers.softmax import softmax_loss_naive\n",
    "from utils.classifiers.softmax import softmax_loss_vectorized\n",
    "\n",
    "## generate a random weight matrix of small numbers\n",
    "np.random.seed(3456)\n",
    "W = np.random.randn(X_train.shape[1], 20) * 0.0001\n",
    "W_tf = tf.Variable(W, dtype = tf.float32)\n",
    "X = tf.Variable(X_dev, dtype = tf.float32)\n",
    "y = tf.Variable(y_dev, dtype = tf.int32)\n",
    "reg = tf.constant(0.000005)\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(W_tf)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits= tf.matmul(X, W_tf), labels=tf.one_hot(y,20))\n",
    "    loss_gt = tf.reduce_mean(cross_entropy) + reg * tf.reduce_sum(W_tf * W_tf)   \n",
    "    grad_gt = tape.gradient(loss_gt, W_tf)\n",
    "\n",
    "## naive softmax in numpy\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive numpy loss: {}, takes {} seconds.'.format(loss_naive, toc-tic))\n",
    "\n",
    "## vectorized softmax in numpy\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized numpy loss: {}, takes {} seconds.'.format(loss_vec, toc-tic))\n",
    "\n",
    "## Verify your result here - use 'rel_err' for error evaluation.\n",
    "def rel_err(a,b):\n",
    "    return np.mean(abs(a-b))\n",
    "\n",
    "print('*'*100)\n",
    "print('Relative loss error of naive softmax is {}'.format(rel_err(loss_gt,loss_naive)))\n",
    "print('Relative loss error of vectorized softmax is {}'.format(rel_err(loss_gt,loss_vec)))\n",
    "print('Gradient error of naive softmax is {}'.format(rel_err(grad_gt,grad_naive)))\n",
    "print('Gradient error of vectorized softmax is {}'.format(rel_err(grad_gt,grad_vec)))\n",
    "print('*'*100)\n",
    "print('Is naive softmax loss correct? {}'.format(np.allclose(loss_gt, loss_naive)))\n",
    "print('Is vectorized softmax loss correct? {}'.format(np.allclose(loss_gt, loss_vec)))\n",
    "print('Is naive softmax grad correct? {}'.format(np.allclose(grad_gt, grad_naive,1e-02)))\n",
    "print('Is vectorized softmax grad correct? {}'.format(np.allclose(grad_gt, grad_vec,1e-02)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1e7981-XHK_f"
   },
   "source": [
    "## Part 3: Train your classifiers\n",
    "\n",
    "Now you can start to train your classifiers. We are going to use gradient descent algorithm for training, which differs from the usual logistic regression training process. \n",
    "\n",
    "<span style=\"color:red\"><strong>TODO</strong></span>: The original code is given in **./utils/classifiers/basic_classifier.py**. You need to complete functions **train** and **predict**, in the class **BasicClassifier**. Later, you use its subclasses **Logistic Regression** and **Softmax** to train the model seperately and verify your result.\n",
    "\n",
    "\n",
    "In the training section, you are asked to implement Stochastic gradient descent (SGD) optimization method. Pseudo code for SGD is shown below.\n",
    "\n",
    "* Stochastic gradient descent - SGD\n",
    "    ```\n",
    "    w = w - learning_rate * gradient \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H35ZjRpZHK_g"
   },
   "source": [
    "### Train Logistic Regression + Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete the code of subclasses **Logistic_Regression** in **./utils/classifiers/basic_classifiers.py**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>NOTE</strong></span>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11111,
     "status": "ok",
     "timestamp": 1559884688501,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "DjRy76pdHK_h",
    "outputId": "ca2707c6-3adf-4bc8-bbce-d30e2a0f0e59",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "from utils.classifiers.basic_classifiers import Logistic_Regression\n",
    "\n",
    "## Logistic Regression + SGD\n",
    "classifier = Logistic_Regression()\n",
    "reg = 1e-5 # regularization\n",
    "lr = 1e-7 # learning rate\n",
    "loss_hist_sgd = classifier.train(X=X_train_binary, y=y_train_binary, learning_rate=lr, reg=reg, num_iters=1500, optim='SGD', verbose=True)\n",
    "\n",
    "# Write the BasicClassifier.predict function and evaluate the performance on both \n",
    "# training set and validation set\n",
    "y_train_pred = classifier.predict(X_train_binary)\n",
    "print('training accuracy: %f' % (np.mean(y_train_binary == y_train_pred), ))\n",
    "y_val_pred = classifier.predict(X_val_binary)\n",
    "print('validation accuracy: %f' % (np.mean(y_val_binary == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1559884692553,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "E_QGDSgvHK_k",
    "outputId": "70dd1a9c-3603-4faf-d8d2-625fbf050c20"
   },
   "outputs": [],
   "source": [
    "## SGD error plot\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HImk5vMHK_n"
   },
   "source": [
    "### Train Softmax + SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete the code of subclasses **Softmax** in **./utils/classifiers/basic_classifier.py**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>NOTE</strong></span>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11222,
     "status": "ok",
     "timestamp": 1559884706216,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "CR4ZDYF6HK_o",
    "outputId": "02212126-294d-4e77-c7d6-b23b94593ab2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "from utils.classifiers.basic_classifiers import Softmax\n",
    "\n",
    "## Softmax + SGD\n",
    "classifier = Softmax()\n",
    "reg = 1e-5 # regularization\n",
    "lr = 1e-7 # learning rate\n",
    "loss_hist_sgd = classifier.train(X=X_train, y=y_train, learning_rate=lr, reg=reg, num_iters=1500, optim='SGD', verbose=True)\n",
    "\n",
    "# Write the BasicClassifier.predict function and evaluate the performance on both the\n",
    "# training and validation set\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 574,
     "status": "ok",
     "timestamp": 1559884709159,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "SCl-qjtlHK_r",
    "outputId": "0d82c109-b1c9-43ea-90c5-6bc6084751af",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## SGD loss curve\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task1-basic_classifiers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
