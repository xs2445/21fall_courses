{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: XNOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 1: Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Consider a simple RNN network shown in the following figure, where _wx, wh, b1, w, b2_ are the scalar parameters of the network. The loss function is the **mean squared error (MSE)**. Given input _(x1, x2) = (-1, 2)_, ground truth _(g1, g2) = (0, 1), h0 = 0, and (wx, wh, b1, w, b2) = (3, 1, 2, -1, 1)_, **compute _(dwx, dwh, db1, dw, db2)_**, which are the gradients of loss with repect to 5 parameters _(wx, wh, b1, w, b2)_.\n",
    "\n",
    "![bptt](./img/bptt2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "\n",
    "Answer the above question. \n",
    "\n",
    "* Show all of your derivations and the computational process.\n",
    "* Use LATEX to edit the equations (Jupyter notebook can recognize the basic LATEX syntax). Alternatively, you can edit equations in some other environment and then paste the screenshot of the equations here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Solution:</span>\n",
    "\n",
    "**[fill in here: Enter your derivations and the computational process]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tensorflow to verify the answer.\n",
    "with tf.GradientTape() as G:\n",
    "    w = tf.Variable(-1.0)\n",
    "    b2 = tf.Variable(1.0)\n",
    "    wx = tf.Variable(3.0)\n",
    "    wh = tf.Variable(1.0)\n",
    "    b1 = tf.Variable(2.0)\n",
    "\n",
    "    h0 = tf.Variable(0.0)\n",
    "    x = tf.Variable((-1.0,2.0))\n",
    "    g = tf.Variable((0.0, 1.0))\n",
    "\n",
    "    y = []\n",
    "    h1 = tf.sigmoid(wx*x[0] + wh*h0 + b1)\n",
    "    y.append(tf.sigmoid(w*h1 + b2))\n",
    "    h2 = tf.sigmoid(wx*x[1] + wh*h1 + b1)\n",
    "    y.append(tf.sigmoid(w*h2 + b2))\n",
    "\n",
    "    loss = 0.5*(tf.square(g[0]-y[0]) + tf.square(g[1]-y[1]))\n",
    "\n",
    "    dw_t, db2_t, dwx_t, dwh_t, db1_t = G.gradient(loss, [w, b2, wx, wh, b1])\n",
    "    \n",
    "\n",
    "print(\"Verified by tensorflow (TODO compare values with your hand-written calculations):\")\n",
    "print(\"dw = {:.4f}, db2 = {:.4f}, dwx = {:.4f}, dwh = {:.4f}, db1 = {:.4f}\".format(dw_t, db2_t, dwx_t, dwh_t, db1_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Use TensorFlow modules to create XNOR network\n",
    "\n",
    "In this part, you need to build and train an XNOR network that can learn the XNOR function. It is a very simple implementation of RNN and will give you an idea how RNN is built and how to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XNOR network\n",
    "\n",
    "XNOR network can learn the XNOR ($\\odot$) function\n",
    "\n",
    "As shown in the figure below, and for instance, if input $(x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7)$=(0,0,1,1,1,1,1,0), then output $(y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8)$=(1,1,0,1,0,1,0,0). That is, $y_n = x_0\\odot x_1 \\odot ... \\odot x_{n-1}\\odot x_{n}$\n",
    "\n",
    "![xnor_net](./img/xnor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data set\n",
    "This function provides a way to generate the data which is needed for the training process. You should utilize it when building your training function for the GRU. Read the source code for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.xnor.dataset import create_xnor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_xnor_dataset(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network using a TensorFlow LSTMCell and GRUCell\n",
    "In this section, you are asked to build a XNOR net using a TensorFlow LSTMCell and a GRUCell. In TensorFlow 2, these two cells are supported by Keras. Please check online documents below.\n",
    "\n",
    "Reference: \n",
    "1. [TensorFlow(Keras): Working with RNNs](https://keras.io/guides/working_with_rnns/)\n",
    "2. [TensorFlow: Recurrent Neural Networks (RNN) with Keras](https://www.tensorflow.org/guide/keras/rnn)\n",
    "3. [TensorFlow LSTM cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "4. [TensorFlow GRU cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\n",
    "5. [TensorFlow: Sequential Model](https://www.tensorflow.org/guide/keras/sequential_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO:</span>\n",
    "Use TensorFlow to build and train your XNOR net. The dataset is already provided. You will do the following:\n",
    "- Learn how to use **tf.keras.layers.LSTM** and **tf.keras.layers.GRU** in TensorFlow(Keras). \n",
    "- Choose appropriate parameters to build a model (Sequential Model in Keras is suggested). \n",
    "- Compile your model with appropriate loss function, optimizer, metrics, etc.\n",
    "- Train your model and see the loss history.\n",
    "\n",
    "Tips: \n",
    "1. Make sure that the shape of your data is corrrect after every step.\n",
    "2. Choose your loss function according to your network design.\n",
    "3. Choose 'accuracy' as your metrics when compiling your model.\n",
    "4. Make sure that names of history for the network with LSTMCell and GRUCell (which you used while training) are the same as the ones in the plotting functions.\n",
    "4. Feel free to ask TAs if you get stuck somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network with LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "in_data, out_data = create_xnor_dataset(1000) # create a dataset with a batch size of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "# TODO: build a network with LSTM cell and train it.#\n",
    "#                                                 #\n",
    "###################################################\n",
    "\n",
    "\n",
    "###################################################\n",
    "# END TODO                                        #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint for creating a model with LSTM cells\n",
    "```\n",
    "model = tf.keras.Sequential() \n",
    "model.add(# insert layer 1)\n",
    "model.add(# insert layer 2) \n",
    "model.add(# insert more layers) \n",
    "model.add(# insert output layer)\n",
    "\n",
    "model.summary() \n",
    "model.compile(loss=#choose your loss function,\n",
    "              optimizer=#choose your optimizer with learning rate,\n",
    "              metrics=['accuracy']) \n",
    "history_LSTM = model.fit(in_data, out_data, batch_size=64, epochs=15) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network with GRU cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data, out_data = create_xnor_dataset(1000) # create a dataset with batch size of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# TODO: build a model with GRU cell and train it. #\n",
    "#                                                 #\n",
    "###################################################\n",
    "\n",
    "\n",
    "###################################################\n",
    "# END TODO                                        #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint for creating a model with GRU cell\n",
    "```\n",
    "model = tf.keras.Sequential()\n",
    "model.add(# insert layer 1)\n",
    "model.add(# insert layer 2)\n",
    "model.add(# insert more layers)\n",
    "model.add(# insert output layer)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=#choose your loss function,\n",
    "              optimizer=#choose your optimizer with learning rate,\n",
    "              metrics=['accuracy'])\n",
    "history_GRU = model.fit(in_data, out_data, batch_size=64, epochs=15)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_LSTM.history['loss'], label='LSTM')\n",
    "plt.plot(history_GRU.history['loss'], label='GRU')\n",
    "plt.title('LSTM/GRU loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_LSTM.history['accuracy'], label='LSTM')\n",
    "plt.plot(history_GRU.history['accuracy'], label='GRU')\n",
    "plt.title('LSTM/GRU accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Answer the question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which part of this task have you been struggling with most of the  time? Describe how you resolved it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__SOLUTION:__</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Answer the question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which loss function did you use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">__SOLUTION:__</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 :  Build your own LSTMCell\n",
    "In this part, you need to build your own LSTM cell to achieve the LSTM functionality (including different types of gates that constitute the cell).\n",
    "\n",
    "You should refer to the materials on how the LSTM cell works  - with its cell states, activation functions and kernels. \n",
    "<br>Please see the course slides or the website https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n",
    "\n",
    "<span style=\"color:red\">TODO:</span> \n",
    "1. Complete the function **LSTM_step** in utils/xnor/LSTM_step.py;\n",
    "2. Verify the function by running the provided code.\n",
    "3. Use the same way (as described above) to build the model and to train it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.xnor.LSTM_step import LSTM_step\n",
    "\n",
    "cell_inputs = np.ones((1,1))\n",
    "cell_states = [0.2*np.ones((1,64)), np.zeros((1,64))]\n",
    "kernel = 0.1*np.ones((1,256))\n",
    "recurrent_kernel = 0.1*np.ones((64,256))\n",
    "bias = np.zeros(256)\n",
    "\n",
    "h , [h,c] = LSTM_step(cell_inputs, cell_states, kernel, recurrent_kernel, bias)\n",
    "print('Simple verification:')\n",
    "print('Is h correct?', np.isclose(h.numpy()[0][0],0.48484358))\n",
    "print('Is c correct?', np.isclose(c.numpy()[0][0],0.70387213))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data, out_data = create_xnor_dataset(1000)# create a dataset with a batch size of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# TODO: build a model with LSTM cell and train it.#\n",
    "#                                                 #\n",
    "###################################################\n",
    "\n",
    "###################################################\n",
    "# END TODO                                        #\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint for building a model with LSTM cell\n",
    "```\n",
    "model = tf.keras.Sequential()\n",
    "model.add(# insert layer 1)\n",
    "model.add(# insert layer 2)\n",
    "model.add(# insert more layers)\n",
    "model.add(# insert output layer)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=#choose your loss function,\n",
    "              optimizer=#choose your optimizer with learning rate,\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(in_data, out_data, batch_size=64, epochs=15)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
