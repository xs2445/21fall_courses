{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "import pyopencl.array as array\n",
    "import matplotlib.pyplot as plt\n",
    "from pyopencl import Buffer, MemoryObject\n",
    "from scipy.signal import convolve2d\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "\tdef __init__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tAttributes for instance of clModule\n",
    "\t\tIncludes OpenCL context, command queue, kernel code.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# Get platform and device property\n",
    "\t\tNAME = 'NVIDIA CUDA'\n",
    "\t\tplatforms = cl.get_platforms()\n",
    "\t\tdevs = None\n",
    "\t\tfor platform in platforms:\n",
    "\t\t\tif platform.name == NAME:\n",
    "\t\t\t\tdevs = platform.get_devices()       \n",
    "\n",
    "\t\t# Create Context:\n",
    "\t\tself.ctx = cl.Context(devs)\n",
    "\n",
    "\t\t# Setup Command Queue:\n",
    "\t\tself.queue = cl.CommandQueue(self.ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "\n",
    "\t\t# kernel - Write your kernel code here or in a .cu file and link it here.\n",
    "\t\tkernel_code = \"\"\"\n",
    "\t\t//****************************************************************************************\n",
    "\t\t// global memory function\n",
    "\n",
    "\t\t__kernel void conv_gpu_naive_openCL(__global float *N, __global float *M, __global float *P, \n",
    "\t\t\t\t\t\t\t\t\t\t\tconst int width, const int height, const int mask_width){\n",
    "\t\t\t\n",
    "\t\t\t// position of threads\n",
    "\t\t\t//const int row = get_local_size(0)*get_gruop_id(0) + get_local_id(0);\n",
    "\t\t\t//const int col = get_local_size(1)*get_gruop_id(1) + get_local_id(1);\n",
    "\t\t\tconst int row = get_global_id(0);\n",
    "\t\t\tconst int col = get_global_id(1);\n",
    "\n",
    "\t\t\t// copy to register\n",
    "\t\t\tconst int mask_w = mask_width;\n",
    "\t\t\tconst int n_w = width;\n",
    "\t\t\tconst int n_h = height;\n",
    "\n",
    "\t\t\t// start point of the kernel\n",
    "\t\t\tconst int col_start = col - mask_w/2;\n",
    "\t\t\tconst int row_start = row - mask_w/2;\n",
    "\n",
    "\t\t\tfloat p_value = 0.0f;\n",
    "\n",
    "\t\t\t// see if the thread in the range of N\n",
    "\t\t\tif((row<n_h)&&(col<n_w)){\n",
    "\n",
    "\t\t\t\tfor(int i=0; i<mask_w; i++){\t\t\t// in y direction of mask\n",
    "\n",
    "\t\t\t\t\tint row_mask = mask_w - 1 - i;\t\t// x coordinate in mask\n",
    "\t\t\t\t\tint row_n = row_start + i;\t\t\t// x coordinate in N\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tfor(int j=0; j<mask_w; j++){\t\t// in x direction of mask\n",
    "\n",
    "\t\t\t\t\t\tint col_mask = mask_w - 1 - j;\t// y coordinate in mask\n",
    "\t\t\t\t\t\tint col_n = col_start + j;\t\t// y coordinate in N\n",
    "\n",
    "\t\t\t\t\t\t// if in the range of N\n",
    "\t\t\t\t\t\tif ((col_n>=0) && (col_n<n_w) && (row_n>=0) && (row_n<n_h)){\n",
    "\t\t\t\t\t\t\tp_value += N[row_n*n_w+col_n] * M[row_mask*mask_w+col_mask];\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t\tP[row*n_w+col] = p_value;\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\n",
    "\t\t//****************************************************************************************\n",
    "\n",
    "\t\t// predefined parameters\n",
    "\t\t#define MASK_SIZE 5\n",
    "\t\t#define TILE_SIZE 12\n",
    "\t\t#define TILE_SIZE_PAD (TILE_SIZE + MASK_SIZE - 1)\n",
    "\n",
    "\t\t//****************************************************************************************\n",
    "\t\t// local memory function (which is shared memory in cuda)\n",
    "\n",
    "\t\t__kernel void conv_gpu_shared_mem_openCL(__global float *N, __global float *M, __global float *P, \n",
    "\t\t\t\t\t\t\t\t\t\t\tconst int width, const int height){\n",
    "\t\t\n",
    "\t\t\t// copy to register\n",
    "\t\t\tconst int n_w = width;\n",
    "\t\t\tconst int n_h = height;\n",
    "\n",
    "\t\t\t__local float N_ds[TILE_SIZE][TILE_SIZE];\t\t// copy input matrix to local memory (which is shared memory in cuda)\n",
    "\t\t\t//__local float M_ds[MASK_SIZE][MASK_SIZE];\t\t// copy mask to lcoal memory\n",
    "\n",
    "\t\t\t// current position of thread\n",
    "\t\t\tconst int tx = get_local_id(0);\n",
    "\t\t\tconst int ty = get_local_id(1);\n",
    "\n",
    "\t\t\t// current position of input matrix\n",
    "\t\t\tconst int col = tx + get_group_id(0)*TILE_SIZE;\n",
    "\t\t\tconst int row = ty + get_group_id(1)*TILE_SIZE;\n",
    "\n",
    "\t\t\t// start point of mask\n",
    "\t\t\tconst int col_start = col - MASK_SIZE/2;\n",
    "\t\t\tconst int row_start = row - MASK_SIZE/2;\n",
    "\n",
    "\t\t\t// copy mask to shared memory\n",
    "\t\t\t//for(int i=0; i<MASK_SIZE; i++){\t\t\t\t// in x direction of mask\n",
    "\t\t\t\t//for(int j=0; j<MASK_SIZE; j++){\t\t\t// in y direction of mask\n",
    "\t\t\t\t\t//M_ds[i][j] = M[i*MASK_SIZE+j];\n",
    "\t\t\t\t//}\n",
    "\t\t\t//}\n",
    "\n",
    "\t\t\t// copy each tiled matrix into local memory\n",
    "\t\t\tif((row_start>=0) && (row_start<n_h) && (col_start>=0) && (col_start<n_w)){\n",
    "\t\t\t\tN_ds[ty][tx] = N[row_start*n_w+col_start];\n",
    "\t\t\t}\n",
    "\t\t\telse{\n",
    "\t\t\t\tN_ds[ty][tx] = 0.0f;\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\t// need to wait all the thread have done copy\n",
    "    \t\tbarrier(CLK_LOCAL_MEM_FENCE);\n",
    "\t\t\t\n",
    "\t\t\tfloat p_value = 0.0f;\n",
    "\t\t\tif((ty<TILE_SIZE) && (tx<TILE_SIZE)){\t\t\t// in range of tile\n",
    "\n",
    "\t\t\t\t// in y direction of mask\n",
    "\t\t\t\tfor(int i=0; i<MASK_SIZE; i++){\n",
    "\t\t\t\t\tint row_n = ty+i;\t\t\t\t\t\t// y coordinate in N_ds\n",
    "\t\t\t\t\tint row_mask = MASK_SIZE - 1 - i;\t\t// y coordinate in mask\n",
    "\n",
    "\t\t\t\t\t// in x direction of mask\n",
    "\t\t\t\t\tfor(int j=0; j<MASK_SIZE; j++){\n",
    "\t\t\t\t\t\tint col_n = tx+j;\t\t\t\t\t// x coordinate in N_ds\n",
    "\t\t\t\t\t\tint col_mask = MASK_SIZE - 1 - j;\t// x coordinate in mask\n",
    "\t\t\t\t\t\tp_value += N_ds[row_n][col_n] * M[row_mask*MASK_SIZE+col_mask];\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t\tif((row<n_h) && (col<n_w)){\t\t\t\t\t// in range of input matrix\n",
    "\t\t\t\t\tP[row*n_w+col] = p_value;\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\n",
    "\n",
    "\t\t//****************************************************************************************\n",
    "\n",
    "\n",
    "\t\t\"\"\" \n",
    "\n",
    "\t\t# Build kernel code\n",
    "\t\tself.prg = cl.Program(self.ctx, kernel_code).build()\n",
    "\t\tself.mask_size = 5\n",
    "\t\tself.tile_size = 12\n",
    "\t\tself.tile_size_pad = self.tile_size + self.mask_size - 1\n",
    "\n",
    "\tdef conv_gpu_naive(self, N, M):\n",
    "\t\t\"\"\"\n",
    "\t\tparallel convolution using global memory\n",
    "\t\tvisit input matrix and mask from global memory in the kernel\n",
    "\t\tmask can be any size\n",
    "\n",
    "\t\tparams:\n",
    "\t\t- N: input matrix\n",
    "\t\t- M: mask\n",
    "\n",
    "\t\treturn:\n",
    "\t\t- P: result\n",
    "\t\t- time\n",
    "\t\t\"\"\"\n",
    "\t\timport time\n",
    "\t\t# start to record\n",
    "\t\tt_start = time.time()\n",
    "\n",
    "\t\theight, width = N.shape\n",
    "\t\tmask_width = M.shape[0]\n",
    "\n",
    "\t\tP = np.empty_like(N)\n",
    "\n",
    "\t\t# device memory allocation\n",
    "\t\tN_d = cl.array.to_device(self.queue, N)\n",
    "\t\tM_d = cl.array.to_device(self.queue, M)\n",
    "\t\t# P_d = cl.array.empty_like(N)\n",
    "\t\tP_d = cl.array.to_device(self.queue, P)\n",
    "\t\t\n",
    "\t\tGlobalSize = (height, width, 1)\n",
    "\t\t# workgroup size\n",
    "\t\t# Local_size = (height//workitem_size+1, width//workitem_size+1,1)\n",
    "\t\tself.prg.conv_gpu_naive_openCL(self.queue, GlobalSize, None, N_d.data, M_d.data, P_d.data, np.int32(width), \n",
    "\t\t\t\t\t\t\t\t\t\tnp.int32(height), np.int32(mask_width))\n",
    "\n",
    "\t\t# wait for execution to complete.\n",
    "\t\tself.queue.finish()\n",
    "\n",
    "\t\t# Copy output from GPU to CPU\n",
    "\t\tP = np.array(P_d.get())\n",
    "\n",
    "\t\t# Record execution time \n",
    "\t\ttime = time.time() - t_start\n",
    "\n",
    "\t\treturn np.array(P), time*1e3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tdef conv_gpu_shared_mem(self, N, M):\n",
    "\t\t\"\"\"\n",
    "\t\tparallel convolution using local memory\n",
    "\t\tvisit input matrix and mask from local memory in the kernel\n",
    "\t\tmask have to be 5*5\n",
    "\n",
    "\t\tparams:\n",
    "\t\t- N: input matrix\n",
    "\t\t- M: mask\n",
    "\n",
    "\t\treturn:\n",
    "\t\t- P: result\n",
    "\t\t- time\n",
    "\t\t\"\"\"\n",
    "\t\timport time\n",
    "\t\t# start to record\n",
    "\t\tt_start = time.time()\n",
    "\n",
    "\t\theight, width = N.shape\n",
    "\n",
    "\t\tP = np.empty_like(N)\n",
    "\n",
    "\t\t# device memory allocation\n",
    "\t\tN_d = cl.array.to_device(self.queue, N)\n",
    "\t\tM_d = cl.array.to_device(self.queue, M)\n",
    "\t\t# P_d = cl.array.empty_like(N)\n",
    "\t\tP_d = cl.array.to_device(self.queue, P)\n",
    "\t\t\n",
    "\t\t# grid size   \n",
    "\t\tGlobalSize = ((height//self.tile_size+1)*self.tile_size_pad, (width//self.tile_size+1)*self.tile_size_pad)\n",
    "\t\t# block size\n",
    "\t\tLocalSize = (self.tile_size_pad, self.tile_size_pad)\n",
    "\n",
    "\t\t# k = self.prg.conv_gpu_shared_mem_openCL\n",
    "\n",
    "\t\t# k.set_arg(0, N_d.data)\n",
    "\t\t# k.set_arg(1, M_d.data)\n",
    "\t\t# k.set_arg(2, P_d.data)\n",
    "\t\t# k.set_arg(3, np.int32(width))\n",
    "\t\t# k.set_arg(4, np.int32(height))\n",
    "\t\t# cl.enqueue_nd_range_kernel(self.queue, k, global_work_size = (height, width, 1), local_work_size = LocalSize)\n",
    "\n",
    "\t\t# print(LocalSize)\n",
    "\t\t# print(GlobalSize)\n",
    "\t\t# workgroup size\n",
    "\t\t# Local_size = (height//workitem_size+1, width//workitem_size+1,1)\n",
    "\t\tevent = self.prg.conv_gpu_shared_mem_openCL(self.queue, GlobalSize, LocalSize, N_d.data, M_d.data, P_d.data, \n",
    "\t\t\t\t\t\t\t\t\t\t\tnp.int32(width), np.int32(height))\n",
    "\n",
    "\t\tevent.wait()\n",
    "\n",
    "\t\t# wait for execution to complete.\n",
    "\t\t# self.queue.finish()\n",
    "\n",
    "\t\t# Copy output from GPU to CPU\n",
    "\t\tP = P_d.get()\n",
    "\n",
    "\t\t# Record execution time \n",
    "\t\ttime = time.time() - t_start\n",
    "\n",
    "\t\treturn np.array(P), time*1e3\n",
    "\n",
    "\tdef conv_gpu_shared_and_constant_mem(self):\n",
    "\t\t# implement this, note you can change the function signature (arguments and return type)\n",
    "\t\tpass\n",
    "\n",
    "\tdef test_conv_pyopencl(self):\n",
    "\t\t# implement this, note you can change the function signature (arguments and return type)\n",
    "\t\tpass\n",
    "\n",
    "\tdef conv_scipy(self, N, M):\n",
    "\t\t\"\"\"\n",
    "\t\tSerial convolution using scipy.signal.convolve2d\n",
    "\n",
    "\t\tparams:\n",
    "\t\t- N: input matrix\n",
    "\t\t- M: mask\n",
    "\n",
    "\t\treturn:\n",
    "\t\t- P: result\n",
    "\t\t- time\n",
    "\t\t\"\"\"\n",
    "\t\tstart = time.time()\n",
    "\n",
    "\t\tP = convolve2d(N.astype(np.float32), M.astype(np.float32), mode='same')\n",
    "\n",
    "\t\treturn P, (time.time()-start)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "LogicError",
     "evalue": "clWaitForEvents failed: <unknown error -9999>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-efadaca42059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# P_cl_naive, t_naive = conver.conv_gpu_naive(N,M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mP_cl_shared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_shared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_gpu_shared_mem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mP_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_scipy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_scipy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-bcf2c26ced22>\u001b[0m in \u001b[0;36mconv_gpu_shared_mem\u001b[0;34m(self, N, M)\u001b[0m\n\u001b[1;32m    249\u001b[0m \t\t\t\t\t\t\t\t\t\t\tnp.int32(width), np.int32(height))\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0;31m# wait for execution to complete.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLogicError\u001b[0m: clWaitForEvents failed: <unknown error -9999>"
     ]
    }
   ],
   "source": [
    "n_size = 1024\n",
    "N = np.random.rand(n_size,n_size).astype(np.float32)\n",
    "M = np.random.rand(5,5).astype(np.float32)\n",
    "\n",
    "conver = Convolution()\n",
    "\n",
    "# P_cl_naive, t_naive = conver.conv_gpu_naive(N,M)\n",
    "P_cl_shared, t_shared = conver.conv_gpu_shared_mem(N,M)\n",
    "\n",
    "P_scipy, t_scipy = conver.conv_scipy(N,M)\n",
    "\n",
    "# print(np.allclose(P_cl_naive, P_scipy))\n",
    "print(np.allclose(P_cl_shared, P_scipy))\n",
    "# print(P_cl_naive)\n",
    "# print(P_scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccacca6f91b32fdd04295702544fc910388b96b420fc8318af0b7a276f724177"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('poclEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
