{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "\tdef __init__(self):\n",
    "\t\t# \"\"\"\n",
    "\t\t# Attributes for instance of EncoderDecoder module\n",
    "\t\t# \"\"\"\n",
    "\t\tself.mod = self.getSourceModule()\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef getSourceModule(self):\n",
    "\t\t# kernel code wrapper\n",
    "\t\tkernelwrapper_naive = \"\"\"\n",
    "\t\t\t#include <stdio.h>\n",
    "\t\t\t__global__ \n",
    "\t\t\tvoid conv_gpu_naive(float *N, float *P, float *M, int height, int width, int mask_width){\n",
    "\n",
    "\t\t\t\t// the coordinate of thread (also coordinate in N or P)\n",
    "\t\t\t\tint col = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "\t\t\t\tint row = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "\n",
    "\t\t\t\t// copy to register\n",
    "\t\t\t\tint mask_w = mask_width;\n",
    "\t\t\t\tint n_w = width;\n",
    "\t\t\t\tint n_h = height;\n",
    "\t\t\t\t// start point of the kernel\n",
    "\t\t\t\tint col_start = col - mask_w/2;\n",
    "\t\t\t\tint row_start = row - mask_w/2;\n",
    "\n",
    "\t\t\t\tfloat p_value = 0.0f;\n",
    "\n",
    "\t\t\t\t// for every pixel in mask\n",
    "\t\t\t\tfor(int i=0; i<mask_w; i++){\n",
    "\t\t\t\t\t// x coordinate in N\n",
    "\t\t\t\t\tint col_i = col_start + i;\n",
    "\t\t\t\t\t// if in the range of N\n",
    "\t\t\t\t\tif(col_i>=0 && col_i<n_w){\n",
    "\t\t\t\t\t\tfor(int j=0; j<mask_w; j++){\n",
    "\t\t\t\t\t\t\t// y coordinate in N\n",
    "\t\t\t\t\t\t\tint row_i = row_start + j;\n",
    "\t\t\t\t\t\t\t//if in the range of N\n",
    "\t\t\t\t\t\t\tif(row_i>=0 && row_i<n_h){\n",
    "\t\t\t\t\t\t\t\tp_value += N[col_i*n_w+row_i] * M[i*mask_w+j];\n",
    "\t\t\t\t\t\t\t\t//int a = col_i*n_w+row_i;\n",
    "\t\t\t\t\t\t\t\t//printf(\"%d\", a);\n",
    "\t\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t\tP[col*n_w+row] = p_value;\n",
    "\t\t\t}\n",
    "\t\t\n",
    "\t\t\"\"\" # you can either use a string or save the kernel in kernel.cu file and reference it here.\n",
    "\t\t# Compile the kernel code when an instance\n",
    "\t\t# of this class is made. \n",
    "\t\treturn SourceModule(kernelwrapper_naive)\n",
    "\n",
    "\tdef getBlockGridDim(self, N, blocksize=32):\n",
    "\t\tBlockDim = (blocksize, blocksize,1)\n",
    "\t\tGridDim = (N.shape[0]//blocksize+1, N.shape[1]//blocksize+1,1)\n",
    "\t\treturn BlockDim, GridDim\n",
    "\n",
    "\tdef conv_gpu_naive(self, N, M):\n",
    "\t\t\"\"\"\n",
    "\t\tconvolution with global memory\n",
    "\t\t:param N: input matrix\n",
    "\t\t:param M: mask\n",
    "\t\t:return:\n",
    "\t\t- out: a tensor with the same shape as x\n",
    "\t\t- cache: (train phase) cache a random dropout mask used in feedforward process\n",
    "\t\t\t\t(test phase) None\n",
    "\t\t\"\"\"\n",
    "\t\t# implement this, note you can change the function signature (arguments and return type)\n",
    "\t\tfunc_conv = self.mod.get_function(\"conv_gpu_naive\")\n",
    "\t\theight, width = N.shape\n",
    "\t\tprint(height,width)\n",
    "\t\tmask_width = M.shape[0]\n",
    "\t\tprint(mask_width)\n",
    "\t\t# the result matrix\n",
    "\t\tP = np.empty_like(N)\n",
    "\t\t# copy to device global memory\n",
    "\t\tN_d = gpuarray.to_gpu(N)\n",
    "\t\tM_d = gpuarray.to_gpu(M)\n",
    "\t\tP_d = gpuarray.to_gpu(P)\n",
    "\t\t# block and grid size\n",
    "\t\tBlockDim, GridDim = self.getBlockGridDim(N)\n",
    "\n",
    "\t\tfunc_conv(N_d, P_d, M_d, np.int32(height), np.int32(width), np.int32(mask_width), block=BlockDim, grid = GridDim)\n",
    "\n",
    "\t\tP = P_d.get()\n",
    "\n",
    "\t\treturn P\n",
    "\n",
    "\n",
    "\tdef conv_gpu_shared_mem(self):\n",
    "\t\t# implement this, note you can change the function signature (arguments and return type)\n",
    "\t\tpass\n",
    "\n",
    "\tdef conv_gpu_shared_and_constant_mem(self):\n",
    "\t\t# implement this, note you can change the function signature (arguments and return type)\n",
    "\t\tpass\n",
    "\n",
    "\tdef test_conv_pycuda(self):\n",
    "\t\t# implement this, note you can change the function signature (arguments and return type)\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "N = np.ones((100,100),dtype=np.float)\n",
    "M = np.array(([0,0,0],[0,1,0],[0,0,0]),dtype=np.float)\n",
    "conver = Convolution()\n",
    "P = conver.conv_gpu_naive(N,M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "[[9.88131292e-324 9.88131292e-324 9.88131292e-324 ... 9.88131292e-324\n",
      "  9.88131292e-324 9.88131292e-324]\n",
      " [9.88131292e-324 9.88131292e-324 9.88131292e-324 ... 9.88131292e-324\n",
      "  9.88131292e-324 9.88131292e-324]\n",
      " [9.88131292e-324 9.88131292e-324 9.88131292e-324 ... 9.88131292e-324\n",
      "  9.88131292e-324 9.88131292e-324]\n",
      " ...\n",
      " [7.49257087e-001 2.01161317e-001 6.67803435e-001 ... 6.85166287e-001\n",
      "  9.31359495e-001 3.19413440e-320]\n",
      " [6.93300502e-310 6.93300502e-310 0.00000000e+000 ... 7.07801409e-001\n",
      "  4.07992940e-001 3.57359531e-001]\n",
      " [3.26685099e-001 7.83306767e-001 2.73801339e-001 ... 2.61866492e-316\n",
      "  6.40236694e+149 2.40362937e-320]]\n"
     ]
    }
   ],
   "source": [
    "print(N)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6\n",
      "(5, 6, 1)\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "N = np.random.rand(5,6)\n",
    "height, width = N.shape\n",
    "print(height, width)\n",
    "BlockDim = (*N.shape,1)\n",
    "print(BlockDim)\n",
    "M = np.array(([0,0,0],[0,1,0],[0,0,0]))\n",
    "print(M)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdf7c51fe71bae416eddcc8e429936b019ff907e8a09932761e48214d3299c84"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('cudaEnv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
