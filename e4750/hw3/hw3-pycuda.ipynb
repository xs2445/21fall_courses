{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "\tdef __init__(self):\n",
    "\t\t# \"\"\"\n",
    "\t\t# Attributes for instance of EncoderDecoder module\n",
    "\t\t# \"\"\"\n",
    "\t\tself.mod = None\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef getSourceModule(self, method):\n",
    "\t\t# kernel code wrapper\n",
    "\t\tkernelwrapper_naive = \"\"\"\n",
    "\t\t\t#include <stdio.h>\n",
    "\t\t\t__global__ \n",
    "\t\t\tvoid conv_gpu_naive(float *N, float *P, float *M, int height, int width, int mask_width){\n",
    "\n",
    "\t\t\t\t// the coordinate of thread (also coordinate in N or P)\n",
    "\t\t\t\tint col = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "\t\t\t\tint row = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "\n",
    "\t\t\t\t// copy to register\n",
    "\t\t\t\tint mask_w = mask_width;\n",
    "\t\t\t\tint n_w = width;\n",
    "\t\t\t\tint n_h = height;\n",
    "\t\t\t\t// start point of the kernel\n",
    "\t\t\t\tint col_start = col - mask_w/2;\n",
    "\t\t\t\tint row_start = row - mask_w/2;\n",
    "\n",
    "\t\t\t\tfloat p_value = 0.0f;\n",
    "\n",
    "\t\t\t\t// in y direction of mask\n",
    "\t\t\t\tfor(int i=0; i<mask_w; i++){\n",
    "\t\t\t\t\t// x coordinate in mask\n",
    "\t\t\t\t\tint row_mask = mask_w - 1 - i;\n",
    "\t\t\t\t\t// x coordinate in N\n",
    "\t\t\t\t\tint row_n = row_start + i;\n",
    "\n",
    "\t\t\t\t\t// in x direction of mask\n",
    "\t\t\t\t\tfor(int j=0; j<mask_w; j++){\n",
    "\t\t\t\t\t\t// y coordinate in mask\n",
    "\t\t\t\t\t\tint col_mask = mask_w - 1 - j;\n",
    "\t\t\t\t\t\t// y coordinate in N\n",
    "\t\t\t\t\t\tint col_n = col_start + j;\n",
    "\n",
    "\t\t\t\t\t\t// if in the range of N\n",
    "\t\t\t\t\t\tif ((col_n>=0) && (col_n<n_w) && (row_n>=0) && (row_n<n_h)){\n",
    "\t\t\t\t\t\t\tp_value += N[row_n*n_w+col_n] * M[row_mask*mask_w+col_mask];\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t\tP[row*n_w+col] = p_value;\n",
    "\t\t\t}\n",
    "\t\t\n",
    "\t\t\"\"\" # you can either use a string or save the kernel in kernel.cu file and reference it here.\n",
    "\t\t# Compile the kernel code when an instance\n",
    "\t\t# of this class is made. \n",
    "\t\tif method == 'naive':\n",
    "\t\t\tmod = SourceModule(kernelwrapper_naive)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Please enter the correct method name! -naive\")\n",
    "\t\t\t\n",
    "\t\tself.mod = mod\n",
    "\n",
    "\tdef getBlockGridDim(self, N, blocksize=32):\n",
    "\t\tBlockDim = (blocksize, blocksize,1)\n",
    "\t\tGridDim = (N.shape[0]//blocksize+1, N.shape[1]//blocksize+1,1)\n",
    "\t\treturn BlockDim, GridDim\n",
    "\n",
    "\tdef conv_gpu_naive(self, N, M):\n",
    "\t\t\"\"\"\n",
    "\t\tconvolution with global memory\n",
    "\t\t:param N: input matrix\n",
    "\t\t:param M: mask\n",
    "\t\t:return:\n",
    "\t\t- out: a tensor with the same shape as x\n",
    "\t\t- cache: (train phase) cache a random dropout mask used in feedforward process\n",
    "\t\t\t\t(test phase) None\n",
    "\t\t\"\"\"\n",
    "\t\t# implement this, note you can change the function signature (arguments and return type)\n",
    "\t\t# convert the datatype\n",
    "\t\tN = N.astype(np.float32)\n",
    "\t\tM = M.astype(np.float32)\n",
    "\n",
    "\t\tself.getSourceModule('naive')\n",
    "\t\tfunc_conv = self.mod.get_function(\"conv_gpu_naive\")\n",
    "\t\theight, width = N.shape\n",
    "\t\t# print(height,width)\n",
    "\t\tmask_width = M.shape[0]\n",
    "\t\t# print(mask_width)\n",
    "\t\t# the result matrix\n",
    "\t\tP = np.empty_like(N)\n",
    "\t\t# copy to device global memory\n",
    "\t\tN_d = gpuarray.to_gpu(N)\n",
    "\t\tM_d = gpuarray.to_gpu(M)\n",
    "\t\tP_d = gpuarray.to_gpu(P)\n",
    "\t\t# block and grid size\n",
    "\t\tBlockDim, GridDim = self.getBlockGridDim(N)\n",
    "\n",
    "\t\tfunc_conv(N_d, P_d, M_d, np.int32(height), np.int32(width), np.int32(mask_width), block=BlockDim, grid = GridDim)\n",
    "\n",
    "\t\tP = P_d.get()\n",
    "\n",
    "\t\treturn P\n",
    "\n",
    "\n",
    "\tdef conv_gpu_shared_mem(self):\n",
    "\t\t# implement this, note you can change the function signature (arguments and return type)\n",
    "\t\tpass\n",
    "\n",
    "\tdef conv_gpu_shared_and_constant_mem(self):\n",
    "\t\t# implement this, note you can change the function signature (arguments and return type)\n",
    "\t\tpass\n",
    "\n",
    "\tdef test_conv_pycuda(self):\n",
    "\t\t# implement this, note you can change the function signature (arguments and return type)\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[[0.65003337 0.00969795 0.51755718 0.12056966 0.98800904]\n",
      " [0.28205336 0.25314011 0.08230002 0.04768282 0.42372167]\n",
      " [0.91771188 0.8662492  0.34550165 0.57290916 0.66774025]\n",
      " [0.12833394 0.32253131 0.23299593 0.2697578  0.20994982]\n",
      " [0.80223568 0.88153013 0.95294165 0.96739439 0.06743692]\n",
      " [0.93834359 0.86327429 0.32940206 0.89496798 0.89563055]] \n",
      "\n",
      "[[0.2531401 0.        0.        0.        0.       ]\n",
      " [0.8662492 0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.        0.       ]] \n",
      "\n",
      "[[0.2531401  0.08230002 0.04768282 0.42372167 0.        ]\n",
      " [0.8662492  0.34550166 0.5729092  0.6677402  0.        ]\n",
      " [0.3225313  0.23299593 0.2697578  0.20994982 0.        ]\n",
      " [0.8815301  0.95294166 0.9673944  0.06743692 0.        ]\n",
      " [0.8632743  0.32940206 0.894968   0.89563054 0.        ]\n",
      " [0.         0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xs2445/envs/cudaEnv/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu(14): warning: variable \"n_h\" was declared but never referenced\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = np.random.rand(6,5)\n",
    "M = np.random.rand(3,3)\n",
    "M = np.array([[1,0,0],[0,0,0],[0,0,0]])\n",
    "# print(M)\n",
    "conver = Convolution()\n",
    "P_cu = conver.conv_gpu_naive(N,M)\n",
    "P_sp = convolve2d(N.astype(np.float32), M.astype(np.float32), mode='same')\n",
    "print(np.allclose(P_cu, P_sp))\n",
    "print(N,'\\n')\n",
    "print(P_cu,'\\n')\n",
    "print(P_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 8 , 18 )    258    24\n",
      "( 8 , 19 )    259    23\n",
      "( 8 , 20 )    260    22\n",
      "( 8 , 21 )    261    21\n",
      "( 8 , 22 )    262    20\n",
      "\n",
      "\n",
      "( 9 , 18 )    288    19\n",
      "( 9 , 19 )    289    18\n",
      "( 9 , 20 )    290    17\n",
      "( 9 , 21 )    291    16\n",
      "( 9 , 22 )    292    15\n",
      "\n",
      "\n",
      "( 10 , 18 )    318    14\n",
      "( 10 , 19 )    319    13\n",
      "( 10 , 20 )    320    12\n",
      "( 10 , 21 )    321    11\n",
      "( 10 , 22 )    322    10\n",
      "\n",
      "\n",
      "( 11 , 18 )    348    9\n",
      "( 11 , 19 )    349    8\n",
      "( 11 , 20 )    350    7\n",
      "( 11 , 21 )    351    6\n",
      "( 11 , 22 )    352    5\n",
      "\n",
      "\n",
      "( 12 , 18 )    378    4\n",
      "( 12 , 19 )    379    3\n",
      "( 12 , 20 )    380    2\n",
      "( 12 , 21 )    381    1\n",
      "( 12 , 22 )    382    0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "for(int i=0; i<mask_w; i++){\n",
    "    // x coordinate in mask\n",
    "    int row_mask = mask_w - 1 - i\n",
    "    // x coordinate in N\n",
    "    int row_n = row_start + i;\n",
    "\n",
    "    for(int j=0; j<mask_w; j++){\n",
    "        // y coordinate in mask\n",
    "        int col_mask = mask_w - 1 - j\n",
    "        // y coordinate in N\n",
    "        int col_n = col_start + j;\n",
    "\n",
    "        // if in the range of N\n",
    "        if(col_n>=0 && col_n<n_w && row_n>=0 && row_n<n_h ){\n",
    "            p_value += N[row_n*n_w+col_n] * M[row_mask*mask_w+col_mask];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}\n",
    "\"\"\"\n",
    "mask_w = 5\n",
    "height, width = 20,30\n",
    "row, col = 10,20\n",
    "row_n_start = row - mask_w//2\n",
    "col_n_start = col - mask_w//2\n",
    "for i in range(mask_w):\n",
    "    row_mask = mask_w - 1 - i\n",
    "    row_n = row_n_start + i\n",
    "    for j in range(mask_w):\n",
    "        col_mask = mask_w - 1 - j\n",
    "        col_n = col_n_start + j\n",
    "        if (row_n>=0) & (row_n<height) & (col_n>=0) & (col_n<width):\n",
    "            # print(row_n,'  ', col_n, '  ', row_mask, '  ', col_mask)\n",
    "            print(\"(\", row_n, ',', col_n, ')', '  ', row_n*width+col_n, '  ', row_mask*mask_w+col_mask)\n",
    "    print('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6\n",
      "(5, 6, 1)\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "N = np.random.rand(5,6)\n",
    "height, width = N.shape\n",
    "print(height, width)\n",
    "BlockDim = (*N.shape,1)\n",
    "print(BlockDim)\n",
    "M = np.array(([0,0,0],[0,1,0],[0,0,0]))\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "# DEVICE SETUP\n",
    "BLOCK_SIZE = 32  # Max 32. 32**2 = 1024, max for GTX1060\n",
    "\n",
    "def getSourceModule():\n",
    "    kernel = \"\"\"\n",
    "    __global__ \n",
    "    void conv(const float *A, const float *B, int aw, int ah, int bw, int bh, int b_sum, float *C){\n",
    "\n",
    "        /*Get row and column to operate on from thread coordinates*/\n",
    "        int tx = threadIdx.x;\n",
    "        int ty = threadIdx.y;\n",
    "        \n",
    "        int bx = blockIdx.x;\n",
    "        int by = blockIdx.y;\n",
    "        \n",
    "        int row = by*blockDim.y + ty;\n",
    "        int col = bx*blockDim.x + tx;\n",
    "        \n",
    "        /*Calculate \"padding\" radius of convolution kernel (distance around central pixel)*/\n",
    "        int pw = (bw-1)/2;\n",
    "        int ph = (bh-1)/2;\n",
    "\n",
    "        /*If within the range of C (ie A - padding)*/\n",
    "        if( row < (ah-2*ph) && col < (aw-2*pw) ) {\n",
    "            \n",
    "            /*Set initial pixel value*/\n",
    "            int val = 0;\n",
    "            \n",
    "            /*For each vertical position on the kernel matrix, relative to the central pixel*/\n",
    "            for(int i=-ph; i<=ph; i=i+1){\n",
    "                /*Calculate zero-indexed row ID on kernel matrix*/\n",
    "                int b_row = i+ph; \n",
    "\n",
    "                /*For each horizontal position on the kernel matrix, relative to the central pixel*/\n",
    "                for(int j=-pw; j<=pw; j=j+1){\n",
    "                    /*Calculate zero-indexed column ID on kernel matrix*/\n",
    "                    int b_col = j+pw;\n",
    "\n",
    "                    /*Add product of kernel value and corresponding image value to running total*/\n",
    "                    val += A[ (row+ph +i)*aw + (col+pw +j) ] * B[ b_row*bw + b_col ];\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            /*Copy appropriately normalised resulting pixel value to position on C matrix*/\n",
    "            C[row*(aw-2*pw) + col] = val/b_sum;\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    return SourceModule(kernel)\n",
    "    \n",
    "# Compile kernel\n",
    "mod = getSourceModule()\n",
    "\n",
    "# Get functions\n",
    "conv = mod.get_function(\"conv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convolve(a, b):\n",
    "    global BLOCK_SIZE\n",
    "    global conv\n",
    "    \n",
    "    a, b = [np.array(i).astype(np.float32) for i in [a, b]]\n",
    "    \n",
    "    # Matrix A \n",
    "    aw = np.int32(a.shape[1])  # Widthof in matrix\n",
    "    ah = np.int32(a.shape[0])  # Height of in matrix\n",
    "    \n",
    "    # Matrix B (kernel)\n",
    "    bw = np.int32(b.shape[1])  # Widthof in matrix\n",
    "    if bw % 2 == 0:\n",
    "        print(\"Kernel width is not an odd number! Strange things will happen...\")\n",
    "    bh = np.int32(b.shape[0])  # Height of in matrix\n",
    "    if bh % 2 == 0:\n",
    "        print(\"Kernel height is not an odd number! Strange things will happen...\")\n",
    "    b_sum = np.int32(np.absolute(b).sum())\n",
    "    \n",
    "    # Matrix C, subtract 2*padding, *2 because it's taken off all sides\n",
    "    c = np.empty([ah-(bh-1), aw-(bw-1)])\n",
    "    c = c.astype(np.float32)\n",
    "    \n",
    "    # Allocate memory on device\n",
    "    a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "    b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "    c_gpu = cuda.mem_alloc(c.nbytes)\n",
    "    \n",
    "    # Copy matrix to memory\n",
    "    cuda.memcpy_htod(a_gpu, a)\n",
    "    cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "    # Set grid size from A matrix\n",
    "    grid = (int(aw/BLOCK_SIZE+(0 if aw % BLOCK_SIZE is 0 else 1)), \n",
    "            int(ah/BLOCK_SIZE+(0 if ah % BLOCK_SIZE is 0 else 1)), \n",
    "                          1)\n",
    "    \n",
    "    # Call gpu function\n",
    "    conv(a_gpu, b_gpu, aw, ah, bw, bh, b_sum, c_gpu, block=(BLOCK_SIZE, BLOCK_SIZE, 1), grid=grid)\n",
    "    \n",
    "    # Copy back the result\n",
    "    cuda.memcpy_dtoh(c, c_gpu)\n",
    "    \n",
    "    # Free memory. May not be useful? Ask about this.\n",
    "    a_gpu.free()\n",
    "    b_gpu.free()\n",
    "    c_gpu.free()\n",
    "    \n",
    "    # Return the result\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.23089391 0.12648244 9.84181206 ... 4.38169318 9.23371247 8.44490616]\n",
      " [9.0537361  1.22756095 1.12970777 ... 1.99929716 8.66201555 1.32353132]\n",
      " [2.44116335 6.99184915 3.61355678 ... 8.37585567 1.13886745 2.69067491]\n",
      " ...\n",
      " [0.43791003 3.87850462 1.00171959 ... 4.45572115 3.38204623 7.43212451]\n",
      " [1.80912737 0.2074232  5.94952211 ... 9.50651707 7.78914735 1.64754239]\n",
      " [1.48632327 3.8389076  7.07190159 ... 5.33207501 1.98681875 7.21444586]] \n",
      "\n",
      "[[3. 3. 5. ... 5. 4. 4.]\n",
      " [3. 4. 4. ... 4. 6. 4.]\n",
      " [2. 3. 3. ... 5. 6. 5.]\n",
      " ...\n",
      " [4. 5. 5. ... 5. 5. 5.]\n",
      " [3. 4. 4. ... 5. 5. 5.]\n",
      " [2. 3. 5. ... 4. 5. 6.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = np.random.rand(30,40)*10\n",
    "M = np.random.rand(3,3)*10\n",
    "# M = np.array([[1,0,0],[0,0,0],[0,0,0]])\n",
    "# print(M)\n",
    "P_cu = convolve(N,M)\n",
    "# P_sp = convolve2d(N.astype(np.float32), M.astype(np.float32), mode='full')\n",
    "# print(np.allclose(P_cu, P_sp))\n",
    "print(N,'\\n')\n",
    "print(P_cu,'\\n')\n",
    "# print(P_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdf7c51fe71bae416eddcc8e429936b019ff907e8a09932761e48214d3299c84"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('cudaEnv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
