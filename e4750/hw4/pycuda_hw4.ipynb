{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixSum:\n",
    "    def __init__(self):\n",
    "        self.mod = None\n",
    "        self.getSourceModule()\n",
    "\n",
    "\n",
    "    def getSourceModule(self):\n",
    "        kernelwrapper = \"\"\"\n",
    "        //------------------------------------------------------------------------------\n",
    "        // predefined parameters\n",
    "        #define SECTION_SIZE 1024\n",
    "\n",
    "        //------------------------------------------------------------------------------\n",
    "\n",
    "        // Kogge-Stone scan kernel, naive-gpu\n",
    "        __global__\n",
    "        void KoggeStone(float *X, float *Y, const int InputSize){\n",
    "            \n",
    "            // copy the input array to shared memory\n",
    "            __shared__ float XY[SECTION_SIZE];\n",
    "\n",
    "            const int tx = threadIdx.x;\n",
    "            const int i = blockDim.x*blockIdx.x + tx;\n",
    "\n",
    "            if(i<InputSize){\n",
    "                XY[tx] = X[i];\n",
    "            }\n",
    "\n",
    "            // iterative scan on XY\n",
    "            float temp = 0.0f;\n",
    "            for(unsigned int stride = 1; stride < blockDim.x; stride*=2){\n",
    "                temp = 0;\n",
    "                __syncthreads();\n",
    "                if(tx >= stride) temp = XY[tx-stride];\n",
    "                __syncthreads();\n",
    "                XY[tx] += temp;\n",
    "                //if(tx >= stride) XY[tx] += XY[tx-stride];\n",
    "            }\n",
    "            __syncthreads();\n",
    "        \n",
    "            if(i<InputSize) Y[i] = XY[tx];\n",
    "            //Y[i] = XY[tx];\n",
    "\n",
    "        }\n",
    "\n",
    "    \n",
    "        //------------------------------------------------------------------------------\n",
    "\n",
    "        // Kogge-Stone scan kernel for arbitrary input length, naive-gpu\n",
    "        // phase1&2 for inefficient scan\n",
    "        __global__\n",
    "        void KoggeStone_end(double *X, double *Y, double *end_ary, const int InputSize){\n",
    "            \n",
    "            // copy the input array to shared memory\n",
    "            __shared__ double XY[SECTION_SIZE];\n",
    "\n",
    "            const int tx = threadIdx.x;\n",
    "            const unsigned long int i = blockDim.x*blockIdx.x + tx;\n",
    "\n",
    "            if(i<InputSize){\n",
    "                XY[tx] = X[i];\n",
    "            }\n",
    "\n",
    "            // iterative scan on XY\n",
    "            float temp = 0.0f;\n",
    "            for(unsigned int stride = 1; stride < blockDim.x; stride*=2){\n",
    "                temp = 0;\n",
    "                __syncthreads();\n",
    "                if(tx >= stride) temp = XY[tx-stride];\n",
    "                __syncthreads();\n",
    "                XY[tx] += temp;\n",
    "                //if(tx >= stride) XY[tx] += XY[tx-stride];\n",
    "            }\n",
    "            __syncthreads();\n",
    "        \n",
    "            if(i<InputSize) Y[i] = XY[tx];\n",
    "            //Y[i] = XY[tx];\n",
    "\n",
    "            // copy the last element of the block\n",
    "            if(tx == SECTION_SIZE-1){\n",
    "                end_ary[blockIdx.x] = XY[tx];\n",
    "            }\n",
    "            else if(i == InputSize-1){\n",
    "                end_ary[blockIdx.x] = XY[tx];\n",
    "            }\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "        //------------------------------------------------------------------------------\n",
    "        // phase 3 of inefficient scan\n",
    "\n",
    "        __global__\n",
    "        void phase3_koggestone(double *Y, double *S, const int InputSize){\n",
    "\n",
    "            // current position in Y\n",
    "            const unsigned long int y = blockDim.x*blockIdx.x + threadIdx.x;\n",
    "\n",
    "            if(blockIdx.x > 0 && y<InputSize) Y[y] += S[blockIdx.x-1];\n",
    "            \n",
    "        }\n",
    "\n",
    "\n",
    "        //------------------------------------------------------------------------------\n",
    "\n",
    "        #define SECTION_SIZE_2 SECTION_SIZE*2\n",
    "\n",
    "        //------------------------------------------------------------------------------\n",
    "\n",
    "        // Brent-Kung scan kernel phase1\n",
    "\n",
    "        __global__ \n",
    "        void BrentKung(float *X, float *Y, const int InputSize){\n",
    "\n",
    "            __shared__ float XY[SECTION_SIZE_2];\n",
    "\n",
    "            // current position in block\n",
    "            const int tx = threadIdx.x;\n",
    "            // current position in X\n",
    "            const int i = 2*blockDim.x*blockIdx.x + tx;\n",
    "\n",
    "            if(i<InputSize) XY[tx] = X[i];\n",
    "            if(i+blockDim.x < InputSize) XY[tx+blockDim.x] = X[i+blockDim.x];\n",
    "\n",
    "            // reduction tree\n",
    "            for(unsigned int stride=1; stride <= blockDim.x; stride*=2){\n",
    "                __syncthreads();\n",
    "                int index = (tx+1)*2*stride-1;\n",
    "                if(index<SECTION_SIZE_2) XY[index] += XY[index - stride];                 \n",
    "            }\n",
    "\n",
    "            // distribution tree \n",
    "            for(unsigned int stride=SECTION_SIZE_2/4; stride > 0; stride /=2){\n",
    "                __syncthreads();\n",
    "                int index = (tx+1)*2*stride-1;\n",
    "                if(index+stride < SECTION_SIZE_2) XY[index+stride] += XY[index];\n",
    "            }\n",
    "\n",
    "            __syncthreads();\n",
    "\n",
    "            if(i<InputSize) Y[i] = XY[tx];\n",
    "            if(i+blockDim.x < InputSize) Y[i+blockDim.x] = XY[tx+blockDim.x];\n",
    "\n",
    "        }\n",
    "        \n",
    "        //------------------------------------------------------------------------------\n",
    "        \n",
    "        // Brent-Kung scan kernel phase2\n",
    "        \n",
    "        __global__ \n",
    "        void BrentKung_phase2(float *Y, float *S, const int InputSize){\n",
    "\n",
    "            __shared__ float XY[SECTION_SIZE_2];\n",
    "\n",
    "            // current position in block\n",
    "            const int tx = threadIdx.x;\n",
    "            // current position in S\n",
    "            const int k = blockDim.x*blockIdx.x + tx;\n",
    "            // current position in Y\n",
    "            const int i = (2*k+1)*SECTION_SIZE_2-1;\n",
    "\n",
    "            if(i<InputSize) XY[tx] = Y[i];\n",
    "            if(i+SECTION_SIZE_2 < InputSize) XY[tx+SECTION_SIZE_2] = Y[i+SECTION_SIZE_2];\n",
    "\n",
    "            // reduction tree\n",
    "            for(unsigned int stride=1; stride <= blockDim.x; stride*=2){\n",
    "                __syncthreads();\n",
    "                int index = (tx+1)*2*stride-1;\n",
    "                if(index<SECTION_SIZE_2) XY[index] += XY[index - stride];                 \n",
    "            }\n",
    "\n",
    "            // distribution tree \n",
    "            for(unsigned int stride=SECTION_SIZE_2/4; stride > 0; stride /=2){\n",
    "                __syncthreads();\n",
    "                int index = (tx+1)*2*stride-1;\n",
    "                if(index+stride < SECTION_SIZE_2) XY[index+stride] += XY[index];\n",
    "            }\n",
    "\n",
    "            __syncthreads();\n",
    "\n",
    "            if(i<InputSize) S[i] = XY[tx];\n",
    "            if(i+blockDim.x < InputSize) S[i+blockDim.x] = XY[tx+blockDim.x];\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        //------------------------------------------------------------------------------\n",
    "        // Brent-Kung scan kernel phase1&2 (efficient scan)\n",
    "        // output two array Y: pre-fix sum, end_ary: last elements of each blocok.\n",
    "\n",
    "        __global__ \n",
    "        void BrentKung_end(double *X, double *Y,  double *end_ary, const int InputSize){\n",
    "\n",
    "            __shared__ double XY[SECTION_SIZE_2];\n",
    "\n",
    "            // current position in block\n",
    "            const unsigned int tx = threadIdx.x;\n",
    "            // current position in X\n",
    "            const unsigned long int i = 2*blockDim.x*blockIdx.x + tx;\n",
    "\n",
    "            // each thread copy 2 element to shared memory\n",
    "            if(i<InputSize) XY[tx] = X[i];\n",
    "            if(i+blockDim.x < InputSize) XY[tx+blockDim.x] = X[i+blockDim.x];\n",
    "\n",
    "            // reduction tree\n",
    "            for(unsigned int stride=1; stride <= blockDim.x; stride*=2){\n",
    "                __syncthreads();\n",
    "                int index = (tx+1)*2*stride-1;\n",
    "                if(index<SECTION_SIZE_2) XY[index] += XY[index - stride];                 \n",
    "            }\n",
    "\n",
    "            // distribution tree \n",
    "            for(unsigned int stride=SECTION_SIZE_2/4; stride > 0; stride /=2){\n",
    "                int index = (tx+1)*2*stride-1;\n",
    "                __syncthreads();\n",
    "                if(index+stride < SECTION_SIZE_2) XY[index+stride] += XY[index];\n",
    "            }\n",
    "\n",
    "            __syncthreads();\n",
    "\n",
    "            // output the last element of each block\n",
    "            if(((tx+1) == SECTION_SIZE) && (i+blockDim.x < InputSize)){\n",
    "                end_ary[blockIdx.x] = XY[tx+blockDim.x];\n",
    "            }\n",
    "            else if((i+1) == InputSize){\n",
    "                end_ary[blockIdx.x] = XY[tx];\n",
    "            }\n",
    "            else if((i+blockDim.x+1) == InputSize){\n",
    "                end_ary[blockIdx.x] = XY[tx+blockDim.x];\n",
    "            }\n",
    "            \n",
    "            // output each block\n",
    "            if(i<InputSize) Y[i] = XY[tx];\n",
    "            if(i+blockDim.x < InputSize) Y[i+blockDim.x] = XY[tx+blockDim.x];\n",
    "            \n",
    "        }\n",
    "\n",
    "        //------------------------------------------------------------------------------\n",
    "        // phase 3 of efficient scan\n",
    "\n",
    "        __global__\n",
    "        void phase3_brentkung(double *Y, double *S, const int InputSize){\n",
    "\n",
    "            // current position in Y\n",
    "            const unsigned long int y = 2*(blockDim.x*blockIdx.x) + threadIdx.x;\n",
    "\n",
    "            if(2*blockIdx.x > 0 && y<InputSize) Y[y] += S[blockIdx.x-1];\n",
    "            \n",
    "            if(2*blockIdx.x > 0 && y+blockDim.x < InputSize) Y[y+blockDim.x] += S[blockIdx.x-1];\n",
    "\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "\t\t\n",
    "        mod = SourceModule(kernelwrapper)\n",
    "\n",
    "        self.mod = mod\n",
    "    \n",
    "    @staticmethod\n",
    "    def prefix_sum_python(N, length):\n",
    "        \"\"\"\n",
    "\t\tNaive prefix sum serial implementation\n",
    "        yi = x0 + ... + xi\n",
    "\n",
    "\t\tparams:\n",
    "\t\t- N: input array\n",
    "\n",
    "\t\treturn:\n",
    "        - Y: output array\n",
    "\t\t\"\"\"\n",
    "\n",
    "        Y = np.zeros_like(N)\n",
    "\n",
    "        for i in range(length):\n",
    "            sum_temp = 0\n",
    "            for j in range(i+1):\n",
    "                sum_temp += N[j]\n",
    "            Y[i] = sum_temp\n",
    "        \n",
    "        return Y\n",
    "\n",
    "    @staticmethod\n",
    "    def prefix_sum_python2(N, length):\n",
    "        Y = np.zeros_like(N)\n",
    "\n",
    "        Y[0] = N[0]\n",
    "        for i in range(length-1):\n",
    "            Y[i+1] = Y[i] + N[i+1]\n",
    "\n",
    "        return Y\n",
    "\n",
    "    def prefix_sum_gpu_naive(self, N, length):\n",
    "        \"\"\"\n",
    "        Prefix_sum using Kogge-Stone scan kernel\n",
    "\n",
    "\t\tparams:\n",
    "\t\t- N: input array\n",
    "\n",
    "\t\treturn:\n",
    "\t\t- Y: result\n",
    "\t\t- time\n",
    "\t\t\"\"\"\n",
    "\n",
    "        Y = np.zeros_like(N)\n",
    "\n",
    "        # memory allocation\n",
    "        N_d = gpuarray.to_gpu(N)\n",
    "        Y_d = gpuarray.to_gpu(Y)\n",
    "\n",
    "        # block and grid size\n",
    "        blockdim = 1024\n",
    "        BlockDim = (blockdim, 1, 1)\n",
    "        GridDim = (length//blockdim+1, 1, 1)\n",
    "\n",
    "        func = self.mod.get_function(\"KoggeStone\")\n",
    "        func(N_d, Y_d, np.int32(length), block=BlockDim, grid=GridDim)\n",
    "\n",
    "        # cuda.memcpy_dtoh(Y, Y_d)\n",
    "        Y = Y_d.get()\n",
    "        \n",
    "        return Y\n",
    "\n",
    "\n",
    "    def prefix_sum_gpu_naive2(self, N, length):\n",
    "        \"\"\"\n",
    "        Prefix_sum using Kogge-Stone scan kernel\n",
    "\n",
    "\t\tparams:\n",
    "\t\t- N: input array\n",
    "\n",
    "\t\treturn:\n",
    "\t\t- Y: result\n",
    "\t\t- time\n",
    "\t\t\"\"\"\n",
    "\n",
    "        if length<=1024:\n",
    "            n = 1\n",
    "        else:\n",
    "            n = math.ceil(math.log(length, 1024))\n",
    "        print(n)\n",
    "\n",
    "        Y_d_list = []\n",
    "        E_d_list = [gpuarray.to_gpu(N)]\n",
    "\n",
    "        blocksize = 1024\n",
    "        BlockDim = (blocksize, 1, 1)\n",
    "        gridsize_list = [length]\n",
    "\n",
    "        func_red = self.mod.get_function(\"KoggeStone_end\")\n",
    "        func_dis = self.mod.get_function(\"phase3_koggestone\")\n",
    "\n",
    "        # reduction\n",
    "        for i in range(n):\n",
    "\n",
    "            # memory allocation for Y\n",
    "            Y_d_list.append(gpuarray.zeros(gridsize_list[-1], dtype=np.float64))\n",
    "            # gridsize for this step\n",
    "            gridsize_list.append((gridsize_list[i]-1)//blocksize+1)\n",
    "            print(gridsize_list[-1])\n",
    "            # list of last elements for this step\n",
    "            E_d_list.append(gpuarray.zeros(gridsize_list[-1], dtype=np.float64))\n",
    "\n",
    "            func_red(E_d_list[i], Y_d_list[i], E_d_list[-1], np.int32(gridsize_list[i]), block=BlockDim, grid=(gridsize_list[-1],1,1))\n",
    "            cuda.Context.synchronize()\n",
    "\n",
    "        # distribution\n",
    "        for i in range(n-1)[::-1]:\n",
    "            func_dis(Y_d_list[i], Y_d_list[i+1], np.int32(gridsize_list[i]), block=BlockDim, grid=(gridsize_list[i+1],1,1))\n",
    "            cuda.Context.synchronize()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Y = Y_d_list[0].get().copy()\n",
    "        E = E_d_list[1].get().copy()\n",
    "        \n",
    "        return Y, E\n",
    "\n",
    "\n",
    "    def prefix_sum_gpu_work_efficient(self, N, length):\n",
    "        \"\"\"\n",
    "        Prefix_sum using Brent-Kung scan kernel\n",
    "\n",
    "\t\tparams:\n",
    "\t\t- N: input array\n",
    "\n",
    "\t\treturn:\n",
    "\t\t- Y: result\n",
    "\t\t- time\n",
    "\t\t\"\"\"\n",
    "        if length<=2048:\n",
    "            n = 1\n",
    "        else:\n",
    "            n = math.ceil(math.log(length, 2048))\n",
    "\n",
    "        Y_d_list = []\n",
    "        E_d_list = [gpuarray.to_gpu(N)]\n",
    "\n",
    "        blocksize = 1024\n",
    "        BlockDim = (blocksize, 1, 1)\n",
    "        gridsize_list = [length]\n",
    "\n",
    "        # function of reduction kernel\n",
    "        func_red = self.mod.get_function(\"BrentKung_end\")\n",
    "        # function of distribution kernel\n",
    "        func_dis = self.mod.get_function(\"phase3_brentkung\")\n",
    "\n",
    "        # reduction\n",
    "        for i in range(n):\n",
    "\n",
    "            # memory allocation for Y\n",
    "            Y_d_list.append(gpuarray.zeros(gridsize_list[-1], dtype=np.float64))\n",
    "            # gridsize for this step\n",
    "            gridsize_list.append((gridsize_list[i]-1)//(blocksize*2)+1)\n",
    "            # list of last elements for this step\n",
    "            E_d_list.append(gpuarray.zeros(gridsize_list[-1], dtype=np.float64))\n",
    "\n",
    "            func_red(E_d_list[i], Y_d_list[i], E_d_list[-1], np.int32(gridsize_list[i]), block=BlockDim, grid=(gridsize_list[-1],1,1))\n",
    "            cuda.Context.synchronize()\n",
    "\n",
    "        # distribution\n",
    "        for i in range(n-1)[::-1]:\n",
    "            func_dis(Y_d_list[i], Y_d_list[i+1], np.int32(gridsize_list[i]), block=BlockDim, grid=(gridsize_list[i+1],1,1))\n",
    "            cuda.Context.synchronize()\n",
    "\n",
    "        Y = Y_d_list[0].get().copy()\n",
    "\n",
    "        return Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def prefix_sum_gpu_work_efficient2(self, N, length):\n",
    "        \"\"\"\n",
    "        Prefix_sum using Brent-Kung scan kernel\n",
    "\n",
    "\t\tparams:\n",
    "\t\t- N: input array\n",
    "\n",
    "\t\treturn:\n",
    "\t\t- Y: result\n",
    "\t\t- time\n",
    "\t\t\"\"\"\n",
    "\n",
    "        Y = np.zeros_like(N)\n",
    "\n",
    "        N_d = gpuarray.to_gpu(N)\n",
    "        Y_d = gpuarray.to_gpu(Y)\n",
    "\n",
    "        # block and grid size\n",
    "        blockdim = 1024\n",
    "        BlockDim = (blockdim, 1, 1)\n",
    "        gridsize = (length-1)//(blockdim*2)+1\n",
    "        GridDim = (gridsize, 1, 1)\n",
    "\n",
    "        # end elements of each blocks\n",
    "        end_ary = np.zeros(gridsize).astype(np.float64)\n",
    "        end_ary_d = gpuarray.to_gpu(end_ary)\n",
    "        # invoke the kernel function \n",
    "        func = self.mod.get_function(\"BrentKung_end\")\n",
    "        func_dis = self.mod.get_function(\"phase3_brentkung\")\n",
    "\n",
    "        func(N_d, Y_d, end_ary_d, np.int32(length), block=BlockDim, grid=GridDim)\n",
    "        cuda.Context.synchronize()\n",
    "        if length > 2048:\n",
    "            Y_phase2 = np.zeros(gridsize).astype(np.float64)\n",
    "            Y_phase2_d = gpuarray.to_gpu(Y_phase2)\n",
    "            gridsize_phase2 = (gridsize-1)//(blockdim*2)+1\n",
    "            GridDim_phase2 = (gridsize_phase2, 1, 1)\n",
    "            end_ary_phase2 = np.zeros(gridsize_phase2).astype(np.float64)\n",
    "            end_ary_phase2_d = gpuarray.to_gpu(end_ary_phase2)\n",
    "            func(end_ary_d, Y_phase2_d, end_ary_phase2_d, np.int32(gridsize), block=BlockDim, grid=GridDim_phase2)\n",
    "            cuda.Context.synchronize()\n",
    "            if length > 2048*2048:\n",
    "                Y_phase3 = np.zeros(gridsize_phase2).astype(np.float64)\n",
    "                Y_phase3_d = gpuarray.to_gpu(Y_phase3)\n",
    "                gridsize_phase3 = (gridsize_phase2-1)//(blockdim*2)+1\n",
    "                GridDim_phase3 = (gridsize_phase3, 1, 1)\n",
    "                end_ary_phase3 = np.zeros(gridsize_phase3).astype(np.float64)\n",
    "                end_ary_phase3_d = gpuarray.to_gpu(end_ary_phase3)\n",
    "                func(end_ary_phase2_d, Y_phase3_d, end_ary_phase3_d, np.int32(gridsize_phase2), block=BlockDim, grid=GridDim_phase3)\n",
    "                cuda.Context.synchronize()\n",
    "                # distribution\n",
    "                func_dis(Y_phase2_d, Y_phase3_d, np.int32(length), block=BlockDim, grid=GridDim_phase2)      \n",
    "                cuda.Context.synchronize()\n",
    "\n",
    "            # distribution\n",
    "            func_dis(Y_d, Y_phase2_d, np.int32(length), block=BlockDim, grid=GridDim)\n",
    "\n",
    "        # cuda.memcpy_dtoh(Y, Y_d)\n",
    "        Y = Y_d.get()\n",
    "        # end_ary = end_ary_d.get() \n",
    "        # end_ary_phase2 = end_ary_phase2_d.get()\n",
    "        \n",
    "        return Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def test_prefix_sum_python(printing=True):\n",
    "        \"\"\"\n",
    "\t\tTest function prefix_sum_python\n",
    "        yi = x0 + ... + xi\n",
    "\n",
    "\t\tparams:\n",
    "\t\t- printing(boolean): print debug information\n",
    "\n",
    "\t\treturn:\n",
    "        - (bool): correct or not\n",
    "\t\t\"\"\"\n",
    "\n",
    "        N = np.arange(1,6,step=1)\n",
    "        check_ary = np.array([1,3,6,10,15])\n",
    "        Y = PrefixSum.prefix_sum_python(N, N.shape[0])\n",
    "\n",
    "        if np.allclose(Y,check_ary):\n",
    "            if printing:\n",
    "                print('Input array: ', N)\n",
    "                print('Output array: ', Y)\n",
    "                print('Check array: ', check_ary)\n",
    "                print('Correct!')\n",
    "                return True\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            if printing:\n",
    "                print('Input array: ', N)\n",
    "                print('Output array: ', Y)\n",
    "                print('Check array: ', check_ary)\n",
    "                print('Incorrect!')\n",
    "                return False\n",
    "            else:\n",
    "                return False           \n",
    "\n",
    "\n",
    "    def test_prefix_sum_gpu_naive(self):\n",
    "        # implement this, note you can change the function signature (arguments and return type)\n",
    "        pass\n",
    "\n",
    "    def test_prefix_sum_gpu_work_efficient(self):\n",
    "        # implement this, note you can change the function signature (arguments and return type)\n",
    "        pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "length = 1000\n",
    "\n",
    "N = np.random.rand(length).astype(np.float32)\n",
    "\n",
    "\n",
    "summer = PrefixSum()\n",
    "Y_gpu_naive = summer.prefix_sum_gpu_naive(N, length)\n",
    "Y_python = summer.prefix_sum_python(N, length)\n",
    "print(np.allclose(Y_gpu_naive, Y_python))\n",
    "# print(np.allclose(y_gpu_))\n",
    "# print(end_ary)\n",
    "# print(Y_gpu_efi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.46840719e-01 1.33265669e+00 1.63186849e+00 ... 6.71041003e+07\n",
      " 6.71041009e+07 6.71041016e+07]\n",
      "67104101.5748475\n"
     ]
    }
   ],
   "source": [
    "length = 2**11 * 2**16\n",
    "\n",
    "N = np.random.rand(length).astype(np.float64)\n",
    "\n",
    "summer = PrefixSum()\n",
    "# Y_gpu_efi= summer.prefix_sum_gpu_work_efficient2(N, length)\n",
    "Y_gpu_efi2 = summer.prefix_sum_gpu_work_efficient(N, length)\n",
    "# print(time.time()-start)\n",
    "# print(Y_gpu_efi[-1])\n",
    "# print(Y_gpu_efi)\n",
    "# print(Y_gpu_efi.shape)\n",
    "# Y_python = summer.prefix_sum_python2(N, length)\n",
    "# print(np.allclose(Y_gpu_efi, Y_gpu_efi2))\n",
    "# print(np.allclose(y_gpu_))\n",
    "# print(end_ary)\n",
    "# print(end_ary_2)\n",
    "# print(Y_gpu_efi)\n",
    "print(Y_gpu_efi2)\n",
    "# print(Y_gpu_efi[-20:])\n",
    "# print(Y_gpu_efi[2047:2050], Y_gpu_efi[-1])\n",
    "# print(Y_python[-1])\n",
    "# print(Y_gpu_efi[1000:1027])\n",
    "# print(Y_python[1000:1027])\n",
    "# print(Y_gpu_efi)\n",
    "# print(Y_python)\n",
    "print(N.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "131072\n",
      "128\n",
      "1\n",
      "[511.91428334 512.69835306 513.54656561 514.33229607 515.05812374\n",
      " 515.79575304 515.97174694 515.99955783 516.08433455 516.7508551\n",
      " 517.08265925 517.95047244 518.56988161 519.2416178  519.39244458]\n",
      "[67113114.12358986 67113114.24657273 67113114.40564233 67113114.8555667\n",
      " 67113115.35630976 67113115.81148148 67113116.64907447 67113116.70665126\n",
      " 67113117.3568334  67113118.26613593]\n",
      "[517.08265925 521.24806188 512.97319641 ... 531.25957416 509.18232564\n",
      " 529.41327933]\n",
      "67113118.08026347\n"
     ]
    }
   ],
   "source": [
    "length = 2**11 * 2**16\n",
    "\n",
    "N = np.random.rand(length).astype(np.float64)\n",
    "\n",
    "summer = PrefixSum()\n",
    "# Y_gpu_efi= summer.prefix_sum_gpu_naive(N, length)\n",
    "Y_gpu_efi2, E = summer.prefix_sum_gpu_naive2(N, length)\n",
    "\n",
    "# print(np.allclose(Y_gpu_efi, Y_gpu_efi2))\n",
    "\n",
    "# print(Y_gpu_efi)\n",
    "print(Y_gpu_efi2[1013:1028])\n",
    "print(Y_gpu_efi2[-10:])\n",
    "print(E)\n",
    "\n",
    "print(N.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9968894804238262"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(2048*2000, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i)\n",
    "for i in range(5)[::-1]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "length = 1000\n",
    "N = np.random.rand(length).astype(np.float32)\n",
    "\n",
    "\n",
    "summer = PrefixSum()\n",
    "Y_gpu_efi = summer.prefix_sum_gpu_work_efficient(N, length)\n",
    "Y_gpu_naive = summer.prefix_sum_gpu_naive(N, length)\n",
    "\n",
    "Y_python = summer.prefix_sum_python2(N, length)\n",
    "\n",
    "\n",
    "print(np.allclose(Y_gpu_naive, Y_python, rtol=1e-5))\n",
    "t = (Y_gpu_naive == Y_python)\n",
    "# print(t)\n",
    "# print(t[t==False].shape)\n",
    "print(np.allclose(Y_gpu_efi, Y_python, rtol=1e-5))\n",
    "# t2 = (Y_gpu_efi == Y_python)\n",
    "\n",
    "# print(t2[t2==False].shape)\n",
    "\n",
    "\n",
    "# print(Y_gpu_naive)\n",
    "# print(Y_gpu_efi)\n",
    "# print(Y_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8667297\n"
     ]
    }
   ],
   "source": [
    "a = Y_gpu_naive[t==False] - Y_python[t==False]\n",
    "print(a.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input array:  [1 2 3 4 5]\n",
      "Output array:  [ 1  3  6 10 15]\n",
      "Check array:  [ 1  3  6 10 15]\n",
      "Correct!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "PrefixSum.test_prefix_sum_python()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global memory occupancy:98.268502% free\n",
      "\n",
      "===Attributes for device 0\n",
      "ASYNC_ENGINE_COUNT:3\n",
      "CAN_MAP_HOST_MEMORY:1\n",
      "CLOCK_RATE:1590000\n",
      "COMPUTE_CAPABILITY_MAJOR:7\n",
      "COMPUTE_CAPABILITY_MINOR:5\n",
      "COMPUTE_MODE:DEFAULT\n",
      "CONCURRENT_KERNELS:1\n",
      "ECC_ENABLED:1\n",
      "GLOBAL_L1_CACHE_SUPPORTED:1\n",
      "GLOBAL_MEMORY_BUS_WIDTH:256\n",
      "GPU_OVERLAP:1\n",
      "INTEGRATED:0\n",
      "KERNEL_EXEC_TIMEOUT:1\n",
      "L2_CACHE_SIZE:4194304\n",
      "LOCAL_L1_CACHE_SUPPORTED:1\n",
      "MANAGED_MEMORY:1\n",
      "MAXIMUM_SURFACE1D_LAYERED_LAYERS:2048\n",
      "MAXIMUM_SURFACE1D_LAYERED_WIDTH:32768\n",
      "MAXIMUM_SURFACE1D_WIDTH:32768\n",
      "MAXIMUM_SURFACE2D_HEIGHT:65536\n",
      "MAXIMUM_SURFACE2D_LAYERED_HEIGHT:32768\n",
      "MAXIMUM_SURFACE2D_LAYERED_LAYERS:2048\n",
      "MAXIMUM_SURFACE2D_LAYERED_WIDTH:32768\n",
      "MAXIMUM_SURFACE2D_WIDTH:131072\n",
      "MAXIMUM_SURFACE3D_DEPTH:16384\n",
      "MAXIMUM_SURFACE3D_HEIGHT:16384\n",
      "MAXIMUM_SURFACE3D_WIDTH:16384\n",
      "MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS:2046\n",
      "MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH:32768\n",
      "MAXIMUM_SURFACECUBEMAP_WIDTH:32768\n",
      "MAXIMUM_TEXTURE1D_LAYERED_LAYERS:2048\n",
      "MAXIMUM_TEXTURE1D_LAYERED_WIDTH:32768\n",
      "MAXIMUM_TEXTURE1D_LINEAR_WIDTH:268435456\n",
      "MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH:32768\n",
      "MAXIMUM_TEXTURE1D_WIDTH:131072\n",
      "MAXIMUM_TEXTURE2D_ARRAY_HEIGHT:32768\n",
      "MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES:2048\n",
      "MAXIMUM_TEXTURE2D_ARRAY_WIDTH:32768\n",
      "MAXIMUM_TEXTURE2D_GATHER_HEIGHT:32768\n",
      "MAXIMUM_TEXTURE2D_GATHER_WIDTH:32768\n",
      "MAXIMUM_TEXTURE2D_HEIGHT:65536\n",
      "MAXIMUM_TEXTURE2D_LINEAR_HEIGHT:65000\n",
      "MAXIMUM_TEXTURE2D_LINEAR_PITCH:2097120\n",
      "MAXIMUM_TEXTURE2D_LINEAR_WIDTH:131072\n",
      "MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT:32768\n",
      "MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH:32768\n",
      "MAXIMUM_TEXTURE2D_WIDTH:131072\n",
      "MAXIMUM_TEXTURE3D_DEPTH:16384\n",
      "MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE:32768\n",
      "MAXIMUM_TEXTURE3D_HEIGHT:16384\n",
      "MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE:8192\n",
      "MAXIMUM_TEXTURE3D_WIDTH:16384\n",
      "MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE:8192\n",
      "MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS:2046\n",
      "MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH:32768\n",
      "MAXIMUM_TEXTURECUBEMAP_WIDTH:32768\n",
      "MAX_BLOCK_DIM_X:1024\n",
      "MAX_BLOCK_DIM_Y:1024\n",
      "MAX_BLOCK_DIM_Z:64\n",
      "MAX_GRID_DIM_X:2147483647\n",
      "MAX_GRID_DIM_Y:65535\n",
      "MAX_GRID_DIM_Z:65535\n",
      "MAX_PITCH:2147483647\n",
      "MAX_REGISTERS_PER_BLOCK:65536\n",
      "MAX_REGISTERS_PER_MULTIPROCESSOR:65536\n",
      "MAX_SHARED_MEMORY_PER_BLOCK:49152\n",
      "MAX_SHARED_MEMORY_PER_MULTIPROCESSOR:65536\n",
      "MAX_THREADS_PER_BLOCK:1024\n",
      "MAX_THREADS_PER_MULTIPROCESSOR:1024\n",
      "MEMORY_CLOCK_RATE:5001000\n",
      "MULTIPROCESSOR_COUNT:40\n",
      "MULTI_GPU_BOARD:0\n",
      "MULTI_GPU_BOARD_GROUP_ID:0\n",
      "PCI_BUS_ID:0\n",
      "PCI_DEVICE_ID:4\n",
      "PCI_DOMAIN_ID:0\n",
      "STREAM_PRIORITIES_SUPPORTED:1\n",
      "SURFACE_ALIGNMENT:512\n",
      "TCC_DRIVER:0\n",
      "TEXTURE_ALIGNMENT:512\n",
      "TEXTURE_PITCH_ALIGNMENT:32\n",
      "TOTAL_CONSTANT_MEMORY:65536\n",
      "UNIFIED_ADDRESSING:1\n",
      "WARP_SIZE:32\n"
     ]
    }
   ],
   "source": [
    "(free,total)=cuda.mem_get_info()\n",
    "print(\"Global memory occupancy:%f%% free\"%(free*100/total))\n",
    "\n",
    "for devicenum in range(cuda.Device.count()):\n",
    "    device=cuda.Device(devicenum)\n",
    "    attrs=device.get_attributes()\n",
    "\n",
    "    #Beyond this point is just pretty printing\n",
    "    print(\"\\n===Attributes for device %d\"%devicenum)\n",
    "    for (key,value) in attrs.items():\n",
    "        print(\"%s:%s\"%(str(key),str(value)))\n",
    "    # print(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdf7c51fe71bae416eddcc8e429936b019ff907e8a09932761e48214d3299c84"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('cudaEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
